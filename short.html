<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Named/Typed Tensors: Brief Overview</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://vanillacss.com/vanilla.css">
      <style>
          body{margin:0 auto;max-width:50rem;}
          @media(max-width:50rem) {
              body {
                  padding: 10px;
              }
          }
      </style>

    <meta charset="utf-8" />
    <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
    <meta name="author" content="David Chiang and Sasha Rush" />
    <title>Named Tensor Notation</title>
    <style type="text/css">
        code{white-space: pre-wrap;}
        span.smallcaps{font-variant: small-caps;}
        span.underline{text-decoration: underline;}
        div.column{display: inline-block; vertical-align: top; width: 50%;}
    </style>
    <script src="/usr/share/javascript/mathjax/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

  <div style="display:none">
  \(
    \require{ams}
    \newcommand{\displaylimits}{}
    \DeclareMathOperator*{\softmax}{softmax}
    \DeclareMathOperator{\tupledom}{dom}
    \DeclareMathOperator{\tupleshape}{ind}
  \)
  </div>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Named/Typed Tensors: Brief Overview</h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#formal-definitions">Formal definitions</a></li>
<li><a href="#references">References</a></li>
</ul>
</nav>
<p>Most papers about neural networks use the notation of vectors and matrices from applied linear algebra. This notation works well with vector spaces, but is ill suited for describing neural networks that operate on tensors with many different axes. Consider the following equation <span class="citation" data-cites="vaswani+:2017">(Vaswani et al. 2017)</span>: <span class="math display">\[\text{Attention}(Q, K, V) = \mathop{\mathrm{softmax}}\left( \frac{QK^\top}{\sqrt{d_k}} \right) V. 
  \label{eq:attention-naive}\]</span> where <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, and <span class="math inline">\(V\)</span> are sequences of query, key, and value vectors packed into matrices. Does the product <span class="math inline">\(QK^\top\)</span> sum over the sequence, or over the query/key features? We would need to know the sizes of <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, and <span class="math inline">\(V\)</span> to know that it’s taken over the query/key features. Is the softmax taken over the query sequence or the key sequence? The usual notation doesn’t even offer a way to answer this question. With multiple attention heads, the notation becomes more complicated and leaves more questions unanswered. With multiple sentences in a minibatch, the notation becomes more complicated still, and most papers wisely leave this detail out. As a preview, in our notation the above equation becomes</p>
<p><span class="math display">\[\text{Attention}(Q^{\ensuremath{\mathsf{key}}},K^{\ensuremath{\mathsf{seq}},\ensuremath{\mathsf{key}}},V^{\ensuremath{\mathsf{seq}}}) = \mathop{\text{softmax}}\limits_{\ensuremath{\mathsf{}}} \left( \frac{Q \mathbin{\mathop{\boldsymbol\cdot}\limits_{\ensuremath{\mathsf{key}}}} K}{\sqrt{|\ensuremath{\mathsf{key}}|}} \right) \mathbin{\mathop{\boldsymbol\cdot}\limits_{\ensuremath{\mathsf{seq}}}} V  
\label{eq:attention-new}\]</span> making it unambiguous the types of the tensors, and which axis each operation applies to. The same equation works <em>unchanged</em> when we let <span class="math inline">\(Q\)</span> be a sequence of vectors in <span class="math inline">\(\mathbb{R}^{\ensuremath{\mathsf{key}}}\)</span> rather than a single vector and make the values in <span class="math inline">\(V\)</span> be vectors rather than scalar. It also extends to multiple heads and minibatching.</p>
<p>We now explain our notation using a running example. Consider an image that is modeled as an order-three tensor <span class="math inline">\(I\)</span> that maps a triple <span class="math inline">\((x,y,c)\)</span> into a real number corresponding to the intensity of the channel <span class="math inline">\(c\in \{ \text{`R&#39;}, \text{`G&#39;}, \text{`B&#39;} \}\)</span> at the pixel <span class="math inline">\((x,y)\)</span>. In our notation we write this as <span class="math inline">\(I \in \mathbb{R}^{\ensuremath{\mathsf{width}},\ensuremath{\mathsf{height}},\ensuremath{\mathsf{channel}}}\)</span>. The <em>type</em> of <span class="math inline">\(I\)</span> is denoted as <span class="math inline">\(\{ \ensuremath{\mathsf{width}},\ensuremath{\mathsf{height}},\ensuremath{\mathsf{channel}} \}\)</span> and we will sometimes use superscripts such as <span class="math inline">\(I^{\ensuremath{\mathsf{width}},\ensuremath{\mathsf{height}},\ensuremath{\mathsf{channel}}}\)</span> to remind the reader of it. If <span class="math inline">\(x,y,c\)</span> are known to be indices into the <span class="math inline">\(\ensuremath{\mathsf{width}},\ensuremath{\mathsf{height}},\ensuremath{\mathsf{channel}}\)</span> axes respectively, then we can simply write <span class="math inline">\(I_{x,y,c}\)</span> for the corresponding element of <span class="math inline">\(I\)</span>. Order here does not matter and <span class="math inline">\(I_{x,y,c} = I_{y,x,c}=I_{c,x,y}\)</span> etc. If <span class="math inline">\(x,y,c\)</span> do not have such types (e.g., if they are simply integers) then we need to <em>explicitly cast</em> them into these types by writing <span class="math inline">\(I_{\ensuremath{\mathsf{width}}[x],\ensuremath{\mathsf{height}}[y],\ensuremath{\mathsf{channel}}[c]}\)</span>. If we provide fewer indices than the number of axes in <span class="math inline">\(I\)</span> then the result is the restriction of the tensor to these values. For example, <span class="math inline">\(I_{\ensuremath{\mathsf{channel}}[\text{`B&#39;}]}\)</span> is the <span class="math inline">\(\{ \ensuremath{\mathsf{width}} , \ensuremath{\mathsf{height}} \}\)</span> tensor for which every <span class="math inline">\((x,y)\)</span> coordinate is the intensity of the blue channel at the <span class="math inline">\((x,y)\)</span> pixel. We can also write this as <span class="math inline">\(I_{\ensuremath{\mathsf{channel}}=\text{`B&#39;}}\)</span>.</p>
<p>By specifying axes, we can use notation such as <span class="math inline">\(\mathop{\text{mean}}\limits_{\ensuremath{\mathsf{channel}}}(I)\)</span> to denote the <span class="math inline">\(\{ \ensuremath{\mathsf{width}} , \ensuremath{\mathsf{height}} \}\)</span> type tensor where each pixel location <span class="math inline">\((x,y)\)</span> contains the mean pixel intensity across the <span class="math inline">\(\ensuremath{\mathsf{channel}}\)</span> axis. In contrast, <span class="math inline">\(\mathop{\text{mean}}\limits_{\ensuremath{\mathsf{width}},\ensuremath{\mathsf{weight}}}(I)\)</span> denotes the <span class="math inline">\(\{ \ensuremath{\mathsf{channel}} \}\)</span> type tensor that contains the average RGB values across all pixels. The same logic extends to any operation on tensors. Consider the “90 degree rotation” operation <span class="math inline">\(ROT\)</span> that takes a tensor <span class="math inline">\(A \in \mathbb{R}^{\ensuremath{\mathsf{width}},\ensuremath{\mathsf{height}}}\)</span> and maps it to the tensor <span class="math inline">\(B \in \mathbb{R}^{\ensuremath{\mathsf{width}},\ensuremath{\mathsf{height}}}\)</span> such that for every <span class="math inline">\((x,y)\)</span> pair of coordinates, <span class="math inline">\(B_{x,y} = A_{y,x}\)</span>. In our notation we write this as follows:</p>
<p><span class="math display">\[ROT(A^{\ensuremath{\mathsf{width}},\ensuremath{\mathsf{height}}}) = B^{\ensuremath{\mathsf{width}},\ensuremath{\mathsf{height}}} \label{eq:rotsig}\]</span></p>
<p>where</p>
<p><span class="math display">\[B_{\ensuremath{\mathsf{width}}[y],\ensuremath{\mathsf{height}}[x]} = A_{x,y} \;\;\; \text{For every } (x,y) \in \ensuremath{\mathsf{width}}(A)\times \ensuremath{\mathsf{height}}(A) \label{eq:rotate}\]</span></p>
<p>Here <span class="math inline">\(\ensuremath{\mathsf{width}}(A)\)</span> is the set of all indices that index into <span class="math inline">\(\ensuremath{\mathsf{width}}\)</span> axis of <span class="math inline">\(A\)</span> (i.e., the elements of <span class="math inline">\(\ensuremath{\mathsf{width}}\)</span> on which the partial map <span class="math inline">\(A\)</span> is defined), and we consider this as a disjoint set from the analog set <span class="math inline">\(\ensuremath{\mathsf{height}}(A)\)</span>. Hence there is no ambiguity as to which point <span class="math inline">\(A_{x,y}\)</span> corresponds to, and the order does not matter (i.e., <span class="math inline">\(A_{x,y}\)</span> is equal to <span class="math inline">\(A_{y,x}\)</span> and in both cases correspond to the value of <span class="math inline">\(A\)</span> at the point where the <span class="math inline">\(\ensuremath{\mathsf{width}}\)</span> axis is <span class="math inline">\(x\)</span> and the <span class="math inline">\(\ensuremath{\mathsf{height}}\)</span> axis is <span class="math inline">\(y\)</span>). If we want <span class="math inline">\(x\)</span> to index into the <span class="math inline">\(\ensuremath{\mathsf{height}}\)</span> axis of <span class="math inline">\(B\)</span> instead of into the <span class="math inline">\(\ensuremath{\mathsf{width}}\)</span> axis then we need to explicitly <em>cast</em> it as is done in (<a href="#eq:rotate" data-reference-type="ref" data-reference="eq:rotate">[eq:rotate]</a>).</p>
<p>The <em>signature</em> of <span class="math inline">\(ROT\)</span>, stated in (<a href="#eq:rotsig" data-reference-type="ref" data-reference="eq:rotsig">[eq:rotsig]</a>), is that it maps a <span class="math inline">\(\{ \ensuremath{\mathsf{width}} , \ensuremath{\mathsf{height}} \}\)</span>-type tensor into a <span class="math inline">\(\{ \ensuremath{\mathsf{width}} , \ensuremath{\mathsf{height}} \}\)</span>-type tensor. If we feed into <span class="math inline">\(ROT\)</span> a tensor <span class="math inline">\(A\)</span> with an extra axis such as <span class="math inline">\(\ensuremath{\mathsf{channel}}\)</span> then it is considered a <em>dangling axis</em> which means that the axis <span class="math inline">\(\ensuremath{\mathsf{channel}}\)</span> is carried through to the output, which will be a tensor <span class="math inline">\(B\)</span> of type <span class="math inline">\(\{ \ensuremath{\mathsf{width}} , \ensuremath{\mathsf{height}}  , \ensuremath{\mathsf{channel}} \}\)</span>, such that for every <span class="math inline">\(c \in \ensuremath{\mathsf{channel}}(A)\)</span>, <span class="math inline">\(B_c = ROT(A_c)\)</span>. That is, the operation ROT is applied independently to every restriction of <span class="math inline">\(A\)</span> to the coordinates where the <span class="math inline">\(\ensuremath{\mathsf{channel}}\)</span> axis equals <span class="math inline">\(c\)</span>. Similarly, if we feed into a tensor <span class="math inline">\(A\)</span> with type <span class="math inline">\(\{ \ensuremath{\mathsf{width}} , \ensuremath{\mathsf{height}} , \ensuremath{\mathsf{channel}} , \ensuremath{\mathsf{batch}} \}\)</span> corresponding to a <em>batch</em> of images then the output type will be <span class="math inline">\(\{ \ensuremath{\mathsf{width}} , \ensuremath{\mathsf{height}} , \ensuremath{\mathsf{channel}} , \ensuremath{\mathsf{batch}} \}\)</span>, with <span class="math inline">\(ROT\)</span> applied to every <span class="math inline">\(\{ \ensuremath{\mathsf{channel}},\ensuremath{\mathsf{batch}} \}\)</span> restriction independently.</p>
<h1 id="formal-definitions">Formal definitions</h1>
<p>We now describe the formal definitions.</p>
<h4 id="axes.">Axes.</h4>
<p>An <em>axis</em> is a set of the form <span class="math inline">\(\{ (\text{name}  ,x ) | x\in X \}\)</span> where <span class="math inline">\(\text{name}\)</span> is a string and <span class="math inline">\(X\)</span> is an ordered set. If we don’t specify the ordered set <span class="math inline">\(X\)</span> then it is assumed to be the set of natural numbers <span class="math inline">\(\mathbb{N}\)</span>. Our convention is that that axes’ names are strings of the form “<span class="math inline">\(\text{type}\)</span>” or “<span class="math inline">\(\text{type}^{\text{\it annotation}}\)</span>”, with the assumption that axes with names such as <span class="math inline">\(\text{layer}^{\text{in}}\)</span> and <span class="math inline">\(\ensuremath{\mathsf{\text{layer}}}^{\text{out}}\)</span> correspond to the same type of indices. The names are always unique, and we use the notation <span class="math inline">\(\ensuremath{\mathsf{type}}^{\text{\it annotation}}\)</span> to denote the unique axis with name <span class="math inline">\(\text{type}^{\text{\it annotation}}\)</span>. For <span class="math inline">\(x\in X\)</span>, we use <span class="math inline">\(\ensuremath{\mathsf{type}}^{\text{\it annotation}}[x]\)</span> to denote the corresponding element <span class="math inline">\((\text{type}^{\text{\it annotation}}, x)\)</span> in the set <span class="math inline">\(\ensuremath{\mathsf{type}}^{\text{\it annotation}}\)</span>. we use notation such as <span class="math inline">\(\ensuremath{\mathsf{ax}}\)</span> or <span class="math inline">\(\ensuremath{\mathsf{ax}}^1,\ensuremath{\mathsf{ax}}^2,\ldots\)</span> to denote generic axes.</p>
<h4 id="tensors.">Tensors.</h4>
<p>A <em>tensor type</em> is a finite set <span class="math inline">\(\mathcal{T} = \{ \ensuremath{\mathsf{ax}}^1,\ldots, \ensuremath{\mathsf{ax}}^d \}\)</span> of axes. A (real-valued) <em>tensor</em> of type <span class="math inline">\(\mathcal{T}\)</span> is a partial map <span class="math inline">\(A:\ensuremath{\mathsf{ax}}^1 \times \ensuremath{\mathsf{ax}}^2 \times \cdots \times \ensuremath{\mathsf{ax}}^d \rightarrow \mathbb{R}\)</span>, where the set of coordinates <span class="math inline">\((i_1,\ldots,i_d)\)</span> on which <span class="math inline">\(A\)</span> is defined is a product set <span class="math inline">\(I_1 \times \cdots I_d\)</span> where <span class="math inline">\(I_j \subseteq \ensuremath{\mathsf{ax}}^j\)</span> for <span class="math inline">\(j=1 \ldots d\)</span>. We write <span class="math inline">\(A(i_1,\ldots,i_d)\)</span> or <span class="math inline">\(A_{i_1,\ldots,i_d}\)</span> for the corresponding coordinate. Since <span class="math inline">\(i_1,\ldots,i_d\)</span> are elements of disjoint sets, we can write the inputs to a tensor in any order without ambiguity.</p>
<h4 id="partial-assignments.">Partial assignments.</h4>
<p>If we supply a partial assignment <span class="math inline">\(I = \{ i_{i_1},\ldots,i_{j_\ell} \}\)</span> to the axes <span class="math inline">\(\ensuremath{\mathsf{ax}}^{j_1},\ldots,\ensuremath{\mathsf{ax}}^{j_\ell}\)</span> with <span class="math inline">\(j_1\,ldots,j_\ell\)</span> being distinct indices in <span class="math inline">\([d]\)</span>, then <span class="math inline">\(A_I\)</span> is the tensor of shape <span class="math inline">\(\mathcal{T}&#39; = \mathcal{T} \setminus \{ \ensuremath{\mathsf{ax}}^{j_1},\ldots,\ensuremath{\mathsf{ax}}^{j_\ell} \}\)</span> that maps every <span class="math inline">\(d-\ell\)</span> set of indices <span class="math inline">\(\{ k_1,\ldots,k_{d-\ell} \}\)</span> which are elements of distinct axes in <span class="math inline">\(\mathcal{T}&#39;\)</span> into <span class="math inline">\(A_{k_1,\ldots,k_{d-\ell}, i_{i_1},\ldots,i_{j_\ell}}\)</span>. If <span class="math inline">\(\mathcal{T} = \{ \ensuremath{\mathsf{ax}}^1,\ldots, \ensuremath{\mathsf{ax}}^d \}\)</span> is a type then we let <span class="math inline">\(\mathbb{R}^{\ensuremath{\mathsf{ax}}^1,\ldots, \ensuremath{\mathsf{ax}}^d }\)</span> denote the set of all tensors of type <span class="math inline">\(\mathcal{T}\)</span>. We denote the set <span class="math inline">\(I_j\)</span> of coordinates of <span class="math inline">\(\ensuremath{\mathsf{ax}}^j\)</span> on which <span class="math inline">\(A\)</span> is defined by <span class="math inline">\(\ensuremath{\mathsf{ax}}^j(A)\)</span>.</p>
<h4 id="operators.">Operators.</h4>
<p>An <em>operator</em> is a (possibly partial) map that takes as input one or more tensors of a specified type, and outputs a tensor of a specified type. The <em>signature</em> of an operator is the types of tensors it takes as input and the type of tensor is output. If we invoke an operator <span class="math inline">\(F\)</span> with <span class="math inline">\(k\)</span> inputs on tensors <span class="math inline">\(A^1,\ldots,A^k\)</span> that contain axes not appearing in the signatures then the result is defined as follows. For every <span class="math inline">\(i\)</span>, we let <span class="math inline">\(\mathcal{T}^i\)</span> be the set of axes that the <span class="math inline">\(i\)</span>-th tensor contains and are missing from the corresponding part of <span class="math inline">\(F\)</span>’s signature. For <span class="math inline">\(F(A^1,\ldots,A^k)\)</span> to be defined we need that <strong>(1)</strong> none of those axes appear in the signature of the output, and <strong>(2)</strong> if an axis <span class="math inline">\(\ensuremath{\mathsf{ax}}\)</span> appears in both <span class="math inline">\(\mathcal{T}^i\)</span> and <span class="math inline">\(\mathcal{T}^j\)</span> then it must hold that <span class="math inline">\(\ensuremath{\mathsf{ax}}(A^i)=\ensuremath{\mathsf{ax}}(A^j)\)</span>. In this case, the output <span class="math inline">\(Y\)</span> of <span class="math inline">\(F(A^1,\ldots,A^k)\)</span> will have, in addition to the axes in its signature, also all the axes in <span class="math inline">\(\mathcal{T}^1 \cup \cdots \cup \mathcal{T}^k\)</span>. For every assignment <span class="math inline">\(I\)</span> to these axes, <span class="math inline">\(Y_I\)</span> equals <span class="math inline">\(F(A^1_{\mathcal{T}^1=I},\ldots, A^k_{\mathcal{T}^k=I})\)</span> where by <span class="math inline">\(A_{\mathcal{T}=I}\)</span> we mean the restriction of <span class="math inline">\(A\)</span> obtained by assigning to every ax in <span class="math inline">\(\mathcal{T}\)</span> the corresponding index in <span class="math inline">\(I\)</span>.</p>
<h4 id="example-broadcasting.">Example: broadcasting.</h4>
<p>Consider the operator <span class="math inline">\(ADD(x,y)=x+y\)</span> whose signature is simply <span class="math inline">\(ADD:\mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}\)</span>. If <span class="math inline">\(A\)</span> is a <span class="math inline">\(\{ \ensuremath{\mathsf{row}} , \ensuremath{\mathsf{col}} \}\)</span> tensor and <span class="math inline">\(B\)</span> is a <span class="math inline">\(\ensuremath{\mathsf{col}}\)</span> tensor, then <span class="math inline">\(ADD(A,B)\)</span> will be defined if <span class="math inline">\(\ensuremath{\mathsf{col}}(A)=\ensuremath{\mathsf{col}}(B)\)</span> and in this case have type <span class="math inline">\(\{ \ensuremath{\mathsf{row}}, \ensuremath{\mathsf{col}} \}\)</span>. For every <span class="math inline">\(i \in \ensuremath{\mathsf{row}}(A)\)</span> and <span class="math inline">\(j\in \ensuremath{\mathsf{col}}(A)=\ensuremath{\mathsf{col}}(B)\)</span>, <span class="math inline">\(ADD(A,B)_{i,j} = A_{i,j}+B_j\)</span>.</p>
<h4 id="preconditions-and-postconditions.">Preconditions and postconditions.</h4>
<p>An operator does not have to be defined on all tensors of a given type, and can have restrictions on the inputs, such as requiring certain inputs to have the same support, requiring the support to be a multiple of a certain number, etc. Since these restrictions can vary greatly in applications, these are not part of the signature of the operator, but rather can be stated as pre-conditions. Similarly, it is useful to state post-conditions on the various axes of tensors.</p>
<h4 id="dropping-trivial-dimensions-convention.">Dropping trivial dimensions convention.</h4>
<p>If a tensor <span class="math inline">\(A\)</span> contains an axis <span class="math inline">\(\ensuremath{\mathsf{ax}}\)</span> with <span class="math inline">\(|\ensuremath{\mathsf{ax}}(A)|=1\)</span>, we say that <span class="math inline">\(\ensuremath{\mathsf{ax}}\)</span> is <em>trivial</em> in <span class="math inline">\(A\)</span>. We make the convention that we do not distinguish between tensors that contain an axis trivially and ones that do not contain it at all. Hence we can add a trivial axis to make a tensor fit the signature of an operation, and we can drop one when we wish to align tensors. This means that for example we do not distinguish between a tensor <span class="math inline">\(A\)</span> of type <span class="math inline">\(\{ \ensuremath{\mathsf{row}} , \ensuremath{\mathsf{col}} \}\)</span> where <span class="math inline">\(|\ensuremath{\mathsf{row}}(A)|=1\)</span> and a tensor of type <span class="math inline">\(\{ \ensuremath{\mathsf{col}} \}\)</span>: both are column vectors.</p>
<h4 id="casting.">Casting.</h4>
<p>If <span class="math inline">\(A\)</span> is a tensor containing axis <span class="math inline">\(\ensuremath{\mathsf{ax}}\)</span> then <span class="math inline">\(A_{\ensuremath{\mathsf{ax}} \rightarrow \ensuremath{\mathsf{ax}}&#39;}\)</span> is the tensor replacing <span class="math inline">\(\ensuremath{\mathsf{ax}}\)</span> with <span class="math inline">\(\ensuremath{\mathsf{ax&#39;}}\)</span>. This transformation also changes the corresponding indices according to the semantic interpretation of the indices. So for example, <span class="math inline">\(\ensuremath{\mathsf{channel}} \rightarrow \ensuremath{\mathsf{layer}}\)</span> might map <span class="math inline">\(( \text{`R&#39;}, \text{`G&#39;}, \text{`B&#39;} )\)</span> to <span class="math inline">\((1,2,3)\)</span>. We can also cast multiple axes into one or vice versa, such as the flattening operations <span class="math inline">\(A_{ \{ \ensuremath{\mathsf{width}},\ensuremath{\mathsf{height}},\ensuremath{\mathsf{channel}} \} \rightarrow \ensuremath{\mathsf{layer}} }\)</span>.</p>
<h4 id="slicing.">Slicing.</h4>
<p>If <span class="math inline">\(\ensuremath{\mathsf{ax}}\)</span> is an ax of the form <span class="math inline">\(\{ (ax,x) | x \in X \}\)</span> and <span class="math inline">\(Y \subseteq X\)</span> then <span class="math inline">\(\ensuremath{\mathsf{ax}}[Y]\)</span> is the subset of <span class="math inline">\(\ensuremath{\mathsf{ax}}\)</span> defined as <span class="math inline">\(\{ (ax,x) | x\in Y \}\)</span>. Since <span class="math inline">\(X\)</span> is well ordered, for every <span class="math inline">\(a,b\)</span> we can use <span class="math inline">\(\ensuremath{\mathsf{ax}}[a..b]\)</span>, <span class="math inline">\(\ensuremath{\mathsf{ax}}[a..]\)</span> or <span class="math inline">\(\ensuremath{\mathsf{ax}}[..b]\)</span> to denote the restrictions of <span class="math inline">\(\ensuremath{\mathsf{ax}}\)</span> to the sets <span class="math inline">\(Y\)</span> of inputs between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, at least <span class="math inline">\(a\)</span>, or at most <span class="math inline">\(b\)</span> respectively.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-vaswani+:2017">
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In <em>Advances in Neural Information Processing Systems</em>, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 30:5998–6008. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>.</p>
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>For concreteness, we use “math style” one-based indexing and inclusive intervals here, and assume the natural numbers <span class="math inline">\(\mathbb{N}\)</span> start with <span class="math inline">\(1\)</span>, and so for an axis <span class="math inline">\(\ensuremath{\mathsf{ax}}\)</span> supported on <span class="math inline">\(\mathbb{N}\)</span>, <span class="math inline">\(\ensuremath{\mathsf{ax}}[..n] = \ensuremath{\mathsf{ax}}[\{1,\ldots,n\}]\)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
