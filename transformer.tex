\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{parskip}
\usepackage[margin=1in]{geometry}
\usepackage{multicol}

\usepackage{namedtensor}

\newcommand{\reals}[0]{\mathbb{R}}
\newcommand{\dmodel}{d_{\text{model}}}
\newcommand{\dff}{d_{\text{ff}}}

\title{Named Tensors: Transformer}
\date{}

\begin{document}
\maketitle

% \begin{multicols}{2}

\paragraph{FFN}
\begin{align*}
X &\in \reals^{\nset{emb}{\dmodel}} \\
W^1 &\in \reals^{\nset{emb}{\dmodel, \nset{hid}}{\dff}} & 
b^1 &\in \reals^{\nset{hid}{\dff}} \\
W^2 & \in \reals^{\nset{hid}{\dff}, \nset{emb}{\dmodel}} & b^2 &\in \reals^{\nset{hid}{\dff}} \\
\text{FFN}(X; W, b) &=  W^2 \ndot{hid} \text{ReLU}(W^1 \ndot{emb} X + b^1) + b^2
\end{align*}

\paragraph{Masked Attention}
\begin{align*} 
Q &\in \reals^{\nset{key}{d_k}, \nset{seq'}{n}}, K \in \reals^{\nset{key}{d_k}, \nset{seq}{n}}\\
V &\in \reals^{\nset{seq}{n}, \nset{val}{d_v}fg
},
M \in \reals^{\name{seq}, \name{seq'}}\\
\text{att}(Q, K, V, M) &=  V \ndot{seq} \nfun{seq}{softmax} \left( \frac{\displaystyle Q \ndot{key} K }{\sqrt{d_k}} + M \right) 
\end{align*}

\paragraph{Multiheaded Self Attention}
\begin{align*}
  W^Q &\in \mathbb{R}^{\nset{head}{h}, \nset{emb}{\dmodel}, \nset{key}{d_k}} \\
  W^K &\in \mathbb{R}^{\nset{head}{h}, \nset{emb}{\dmodel}, \nset{key}{d_k}} \\
  W^V &\in \mathbb{R}^{\nset{head}{h}, \nset{emb}{\dmodel}, \nset{val}{d_k}} \\
  W^O &\in \mathbb{R}^{\nset{head}{h}, \nset{val}{d_k}, \nset{emb}{\dmodel}} \\
  X &\in \mathbb{R}^{\nset{seq}{n}, \nset{emb}{\dmodel}} \\
  \text{MHA}(X; W) &= \nmov{seq'}{seq}{W^O \ndot{head,val} \text{att}(Q, K, V, M)} \\
  Q &= W^Q \ndot{emb} \nmov{seq}{seq'}{X} \\
  K &= W^K \ndot{emb} X \\
  V &= W^V \ndot{emb} X \\
  M_{\nidx{seq'}{i}, \nset{seq}{j}} &= \begin{cases} 0 & j\leq i \\ -\infty & \text{otherwise} \end{cases}   
\end{align*}

\paragraph{Layer Norm}

\begin{align*} 
X &\in \reals^{\nset{emb}{\dmodel}} & \gamma, \beta &\in \reals^{\nset{emb}{\dmodel}} \\
\text{lnorm}(X; \gamma, \beta) &= \frac{X - \nfun{emb}{\text{mean}}(X)}{\sqrt{\nfun{emb}{\text{var}}(X)} + \epsilon} \odot \gamma + \beta 
\end{align*}

\paragraph{Position Encoding}
\begin{align*} 
X &\in \{0, 1\}^{\nset{seq}{n}, \nset{vocab}{b}} & \nsum{vocab} X &= 1\\
E &\in \reals^{\nset{vocab*}{v}, \nset{emb}{\dmodel}} \\
\text{embed}(X; E) &= (E \ndot{vocab} X)\sqrt{\dmodel} + P \\
P &\in \reals^{\nset{seq}{n}, \nset{hidden}{\dmodel}} \\
P_{\nidx{hidden}{i}, \nidx{seq}{p}} &= \begin{cases}
  \sin((p-1) / 10000^{(i-1) / \dmodel}) & \text{$i$ odd} \\ 
  \cos((p-1) / 10000^{(i-2) / \dmodel}) & \text{$i$ even} \\
\end{cases} \\
\end{align*}

\paragraph{Transformer}

\begin{align*} 
I &\in \{0, 1\}^{{\nset{seq}{n}, \nset{vocab}{b}}} & \nsum{vocab} X &= 1\\
X^0 &= \text{embed}(I)\\
T^1 &= \text{lnorm}(\text{MHA}(X^0)) + X^0\\
X^1 &= \text{lnorm}(\text{FFN}(T^1)) + T^1\\
&\vdotswithin{=} \\
T^{L} &= \text{lnorm}(\text{MHA}(X^{L-1})) + X^{L-1}\\
X^{L} &= \text{lnorm}(\text{FFN}(T^L)) + T^L\\
O &= \nfun{vocab}{softmax}(W \ndot{emb} X^L)
\end{align*}


% \end{multicols}

\end{document}
