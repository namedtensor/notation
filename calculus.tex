\newcommand{\inp}[1]{#1^*}
\newcommand{\ddx}[1]{\frac{\partial #1}{\partial X}}
\newcommand{\ddxr}[1]{\frac{\partial #1}{\partial X_{\nmov{\mathcal{S}}{\inp{\mathcal{S}}}}}}

If $f$ is a function from order-$m$ tensors to order-$n$ tensors, the partial derivatives of $f$ (evaluated on a tensor $X$) form an order-$(m+n)$ tensor: $m$ ``input'' axes for the directions in which $X$ could change and $n$ ``output'' axes for the change in $f(X)$.

For example, the derivative of a function from vectors to vectors is a matrix (the Jacobian). But using matrix notation, there are conflicting conventions about whether the first axis is the input axis (``denominator layout'') or the output axis (``numerator layout''). The derivative of a function from vectors to matrices or matrices to vectors cannot be represented as a matrix at all, so one must resort to flattening the matrices into vectors.

With tensors, taking derivatives of higher-order tensors with respect to higher-order tensors is not difficult \citep{laue+:2018}. With named tensors, we get the additional advantage of using names to distinguish input and output axes.

\subsection{Definition}

Let $f \colon \reals^\mathcal{S} \rightarrow \reals^\mathcal{T}$, where $\mathcal{S}$ and $\mathcal{T}$ are orthogonal. Then the derivative of $f$ at $X$ is the tensor with shape $\mathcal{S} \times \mathcal{T}$ such that for all $s \in \rec\mathcal{S}$ and $t \in \rec\mathcal{T}$,
\[\left[\ddx{f(X)} \right]_{s,t} = \frac{\partial f(X)_t}{\partial X_s}.\]

What if $\mathcal{S}$ and $\mathcal{T}$ are not orthogonal? We rename $X$ to a shape $\inp{\mathcal{S}}$ that is orthogonal to $\mathcal{T}$, and take the derivative with respect to the renamed $X$ instead:
\[\left[\ddxr{f(X)} \right]_{\inp{s},t} = \frac{\partial f(X)_t}{\partial X_s}.\]

\subsection{Rules}

To compute the derivative of an expression $U$ with shape $\mathcal{T}$ with respect to a variable $X$ with shape $\mathcal{S}$, we use the method of differentials \citep{magnus+neudecker:1985}. The differential of $U$, written $\partial U$, is a tensor with the same shape as $U$, computed using rules like the following:
\begin{align*}
  \partial f(U) &= f'(U) \odot \partial U && f \colon \reals \rightarrow \reals \\
  \partial (U + V) &= \partial U + \partial V \\
  \partial \nsum{ax} U &= \nsum{ax} \partial U \\
  \partial (U \odot V) &= \partial U \odot V + U \odot \partial V \\
  \partial (U \ndot{ax} V) &= \partial U \ndot{ax} V + U \ndot{ax} \partial V \\
  \partial \left(\frac{U}{V}\right) &= \frac{\partial U \odot V - U \odot \partial V}{V^2} \\
  \partial U_r &= \left[\partial U\right]_r && r \in \rec \mathcal{R}, \mathcal{R} \subseteq \mathcal{U} \\
  \partial U_{\nmov{ax1}{ax2}} &= \left[\partial U\right]_{\nmov{ax1}{ax2}}
\end{align*}
If we obtain an equation of the form
\begin{equation*}
  \partial U = A \ndot{\inp{\mathcal{S}}} \partial X_\nmov{\mathcal{S}}{\inp{\mathcal{S}}} + \text{(terms not depending on $\partial X$)}
\end{equation*}
where $\inp{\mathcal{S}}$ is orthogonal to $\mathcal{T}$, then
\begin{equation*}
  \ddxr{U} = A.
\end{equation*}

\subsection{Examples}

Let's find the differential of the softmax operator.
\begin{align*}
  Y &= \nfun{ax}{softmax} X \\
  \partial Y &= \partial \left(\frac{\exp X}{\nsum{ax} \exp X}\right) \\
    &= \frac{\exp X \odot \partial X \odot \nsum{ax} \exp X - \exp X \odot \nsum{ax} (\exp X \odot \partial X)}{(\nsum{ax} \exp X)^2} \\
  &= Y \odot (\partial X - Y \ndot{ax} \partial X).
\end{align*}

To get the Jacobian, $X$ and $Y$ have the same shape so we have to rename one of them. We choose $Y$:
\begin{align*}
  \partial Y_\nmov{ax}{ax'} &= [Y \odot (\partial X - Y \ndot{ax} \partial X)]_\nmov{ax}{ax'} \\
  &= Y_\nmov{ax}{ax'} \odot (I_\name{ax',ax} \ndot{ax} \partial X - Y \ndot{ax} \partial X) \\
  \ddx {Y_\nmov{ax}{ax'}} &= Y_\nmov{ax}{ax'} \odot (I_\name{ax',ax} - Y).
\end{align*}

To derive the rule for backpropagation, we assume a function $f \colon \reals^\name{ax} \rightarrow \reals$ and differentiate $f(Y)$.
\begin{align*}
  \partial f(Y) &= f'(Y) \ndot{ax} \partial Y \\
  &= f'(Y) \ndot{ax} (Y \odot (\partial X - Y \ndot{ax} \partial X)) \\
  &= \nsum{ax} f'(Y) \odot Y \odot \partial X - \left(\nsum{ax} f'(Y) \odot Y\right) \odot \left(\nsum{ax} Y \odot \partial X\right) \\
  &= \nsum{ax} f'(Y) \odot Y \odot \partial X - \nsum{ax} \left(\nsum{ax} f'(Y) \odot Y\right) \odot Y \odot \partial X \\
  &= \nsum{ax} \left(f'(Y) - \left(\nsum{ax} f'(Y) \odot Y\right)\right) \odot Y \odot \partial X \\
  &= ((f'(Y) - f'(Y) \ndot{ax} Y) \odot Y) \ndot{ax} \partial X \\
  \ddx{f(Y)} &= (f'(Y) - f'(Y) \ndot{ax} Y) \odot Y.
\end{align*}

\subsection{Broadcasting}

If $f \colon \reals^\mathcal{S} \rightarrow \reals^\mathcal{T}$, then recall that $f$ can be extended to $\reals^{\mathcal{S} \cup \mathcal{S^+}}$ where $\mathcal{S}$ and $\mathcal{S^+}$ are orthogonal.

It's more convenient here to notate the derivative of $f$ as $Df$. If $f$ has two arguments, its partial derivatives are $D_1 f$ and $D_2 f$.

Although $Df$ extends to $\reals^{\mathcal{S} \cup \mathcal{S^+}}$ using the usual broadcasting rules, the extension of the derivative is unfortunately not the derivative of the extension. To avoid confusion, write $f^+$ for the extension:
\begin{align*}
  f^+ \colon \reals^{\mathcal{S} \cup \mathcal{S^+}} &\rightarrow \reals^{\mathcal{T} \cup \mathcal{S^+}} \\
  f^+(X)_r &= f(X_r).
\end{align*}
Then the derivative of $f^+$ is:
\begin{align*}
  Df^+ \colon \reals^{\inp{\mathcal{S}} \cup \inp{\mathcal{{S^+}}} \cup \mathcal{T} \cup \mathcal{S^+}} &\rightarrow \reals^{\mathcal{T} \cup \mathcal{S^+}} \\
  Df^+(X) &= Df(X) \odot I_{\mathcal{S}^+}.
\end{align*}  

Similarly, if $f \colon \reals^\mathcal{S} \times \reals^\mathcal{T} \rightarrow \reals^\mathcal{U}$, we can extend $f$ to $f^+ \colon \reals^\mathcal{S \cup S^+} \times \reals^\mathcal{T \cup T^+} \rightarrow \reals^\mathcal{U \cup S^+ \cup T^+}$. Then
\begin{align*}
  D_1 f^+(X, Y) &= D_1 f(X, Y) \odot I_{\mathcal{S}^+} \\
  D_2 f^+(X, Y) &= D_2 f(X, Y) \odot I_{\mathcal{T}^+}.
\end{align*}

