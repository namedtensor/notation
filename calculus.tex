\newcommand{\inp}[1]{#1^*}
\newcommand{\ddx}[1]{\frac{\partial #1}{\partial X}}
\newcommand{\ddxr}[1]{\frac{\partial #1}{\partial X_{\nmov{\mathcal{S}}{\mathcal{S}'}}}}

If $f$ is a function from order-$m$ tensors to order-$n$ tensors, the partial derivatives of $f$ (evaluated on a tensor $X$) form an order-$(m+n)$ tensor: $m$ ``input'' axes for the directions in which $X$ could change and $n$ ``output'' axes for the change in $f(X)$.

For example, the derivative of a function from vectors to vectors is a matrix (the Jacobian). But using matrix notation, there are conflicting conventions about whether the first axis is the input axis (``denominator layout'') or the output axis (``numerator layout''). The derivative of a function from vectors to matrices or matrices to vectors cannot be represented as a matrix at all, so one must resort to flattening the matrices into vectors.

With tensors, taking derivatives of higher-order tensors with respect to higher-order tensors is not difficult \citep{laue+:2018}. With named tensors, we get the additional advantage of using names to distinguish input and output axes.

\subsection{Definition}

Let $f \colon \reals^\mathcal{S} \rightarrow \reals^\mathcal{T}$, where $\mathcal{S}$ and $\mathcal{T}$ are orthogonal. Then the derivative of $f$ at $X$ is the tensor with shape $\mathcal{S} \times \mathcal{T}$ such that for all $s \in \rec\mathcal{S}$ and $t \in \rec\mathcal{T}$,
\begin{equation*}
  \left[\ddx{f(X)} \right]_{s,t} = \frac{\partial f(X)_t}{\partial X_s}.
\end{equation*}
If $X$ and $Y$'s shapes are not orthogonal, we have to rename them so that they are.

\subsection{Rules}

To compute the derivative of an expression $U$ with shape $\mathcal{T}$ with respect to a variable $X$ with shape $\mathcal{S}$, we use the method of differentials \citep{magnus+neudecker:1985}. The differential of $U$, written $\partial U$, is a tensor with the same shape as $U$, computed using rules like the following:
\begin{align*}
  \partial f(U) &= f'(U) \odot \partial U && f \colon \reals \rightarrow \reals \\
  \partial (U + V) &= \partial U + \partial V \\
  \partial \nsum{ax} U &= \nsum{ax} \partial U \\
  \partial (U \odot V) &= \partial U \odot V + U \odot \partial V \\
  \partial (U \ndot{ax} V) &= \partial U \ndot{ax} V + U \ndot{ax} \partial V \\
  \partial \left(\frac{U}{V}\right) &= \frac{\partial U \odot V - U \odot \partial V}{V^2} \\
  \partial U_r &= \left[\partial U\right]_r && r \in \rec \mathcal{R}, \mathcal{R} \subseteq \mathcal{U} \\
  \partial U_{\nmov{ax1}{ax2}} &= \left[\partial U\right]_{\nmov{ax1}{ax2}}
\end{align*}
If we obtain an equation of the form
\begin{equation*}
  \partial U = A \ndot{\mathcal{S}} \partial X + \text{(terms not depending on $\partial X$)}
\end{equation*}
where $\mathcal{S}$ is orthogonal to $\mathcal{T}$, then
\begin{equation*}
  \ddx{U} = A.
\end{equation*}
Either $U$ or $X$ could have renamings applied to them:
\begin{align*}
  \partial U = A \ndot{\mathcal{S}} \partial X_\nmov{\mathcal{S}}{\mathcal{S'}} + \cdots &\Rightarrow \ddxr{U} = A \\
  \partial U_\nmov{\mathcal{T}}{\mathcal{T'}} = A \ndot{\mathcal{S}} \partial X + \cdots &\Rightarrow \ddx{U_\nmov{\mathcal{T}}{\mathcal{T'}}} = A.
\end{align*}
The choice between renaming the denominator or numerator of the derivative is analogous between the choice between numerator or denominator layout (respectively) in matrix calculus. But the way we write the derivative always makes it clear what convention is being used.

A very common case is where we end up with an equation of the form
\begin{equation*}
  \partial U = A \odot \partial X + \cdots.
\end{equation*}
Define $I_\name{ax,ax'}$ to be the generalization of the identity matrix such that $[I_\name{ax,ax'}]_{\nidx{ax}{i},\nidx{ax'}{j}} = \delta(i,j)$. Then
\begin{align*}
  \partial U &= A \odot \partial X & \partial U_\nmov{\mathcal{T}}{\mathcal{T'}} &= A_\nmov{\mathcal{T}}{\mathcal{T'}} \odot \partial X_\nmov{\mathcal{T}}{\mathcal{T'}} \\
  &= A \odot (I_{\mathcal{S},\mathcal{S'}} \ndot{\mathcal{S'}} \partial X_\nmov{\mathcal{S}}{\mathcal{S'}}) &&= A_\nmov{\mathcal{T}}{\mathcal{T'}} \odot (I_{\mathcal{T},\mathcal{T'}} \ndot{\mathcal{T}} \partial X) \\
  &= (A \odot I_{\mathcal{S},\mathcal{S'}}) \ndot{\mathcal{S'}} \partial X_\nmov{\mathcal{S}}{\mathcal{S'}} &&= (A_\nmov{\mathcal{T}}{\mathcal{T'}} \odot I_{\mathcal{T},\mathcal{T'}}) \ndot{\mathcal{T}} \partial X \\
  \ddxr{A} &= A \odot I_{\mathcal{S},\mathcal{S'}} & \ddx{U_\nmov{\mathcal{T}}{\mathcal{T'}}} &= A_\nmov{\mathcal{T}}{\mathcal{T'}} \odot I_{\mathcal{T},\mathcal{T'}}.
\end{align*}
assuming that, in the left column, $U$'s shape is orthogonal to $\mathcal{S'}$, and in the right column, to $\mathcal{T'}$.

\subsection{Examples}

Let's find the differential of the softmax operator.
\begin{align*}
  Y &= \nfun{ax}{softmax} X \\
  \partial Y &= \partial \left(\frac{\exp X}{\nsum{ax} \exp X}\right) \\
    &= \frac{\exp X \odot \partial X \odot \nsum{ax} \exp X - \exp X \odot \nsum{ax} (\exp X \odot \partial X)}{(\nsum{ax} \exp X)^2} \\
  &= Y \odot (\partial X - Y \ndot{ax} \partial X).
\end{align*}

To get the Jacobian, $X$ and $Y$ have the same shape, so we have to rename one of them -- we choose $Y$. Then we manipulate it until it's a contraction with $\partial X$.
\begin{align*}
  \partial Y_\nmov{ax}{ax'} &= [Y \odot (\partial X - Y \ndot{ax} \partial X)]_\nmov{ax}{ax'} \\
  &= Y_\nmov{ax}{ax'} \odot (I_\name{ax',ax} \ndot{ax} \partial X - Y \ndot{ax} \partial X) \\
  \ddx {Y_\nmov{ax}{ax'}} &= Y_\nmov{ax}{ax'} \odot (I_\name{ax',ax} - Y).
\end{align*}

To derive the rule for backpropagation, we assume a function $f \colon \reals^\name{ax} \rightarrow \reals$ and differentiate $f(Y)$. Since $f$ is scalar-valued, there is no name overlap, so no renaming is needed.
\begin{align*}
  \partial f(Y) &= f'(Y) \ndot{ax} \partial Y \\
  &= f'(Y) \ndot{ax} (Y \odot (\partial X - Y \ndot{ax} \partial X)) \\
  &= \nsum{ax} f'(Y) \odot Y \odot \partial X - \left(\nsum{ax} f'(Y) \odot Y\right) \odot \left(\nsum{ax} Y \odot \partial X\right) \\
  &= \nsum{ax} f'(Y) \odot Y \odot \partial X - \nsum{ax} \left(\nsum{ax} f'(Y) \odot Y\right) \odot Y \odot \partial X \\
  &= \nsum{ax} \left(f'(Y) - \left(\nsum{ax} f'(Y) \odot Y\right)\right) \odot Y \odot \partial X \\
  &= ((f'(Y) - f'(Y) \ndot{ax} Y) \odot Y) \ndot{ax} \partial X \\
  \ddx{f(Y)} &= (f'(Y) - f'(Y) \ndot{ax} Y) \odot Y.
\end{align*}

\subsection{Broadcasting}

If $f \colon \reals^\mathcal{S} \rightarrow \reals^\mathcal{T}$, then recall that $f$ can be extended to $\reals^{\mathcal{S} \cup \mathcal{S^+}}$ where $\mathcal{S}$ and $\mathcal{S^+}$ are orthogonal.

It's more convenient here to notate the derivative of $f$ as $Df$. If $f$ has two arguments, its partial derivatives are $D_1 f$ and $D_2 f$.

Although $Df$ extends to $\reals^{\mathcal{S} \cup \mathcal{S^+}}$ using the usual broadcasting rules, the extension of the derivative is unfortunately not the derivative of the extension. To avoid confusion, write $f^+$ for the extension:
\begin{align*}
  f^+ \colon \reals^{\mathcal{S} \cup \mathcal{S^+}} &\rightarrow \reals^{\mathcal{T} \cup \mathcal{S^+}} \\
  f^+(X)_r &= f(X_r).
\end{align*}
Then the derivative of $f^+$ is:
\begin{align*}
  Df^+ \colon \reals^{\inp{\mathcal{S}} \cup \inp{\mathcal{{S^+}}} \cup \mathcal{T} \cup \mathcal{S^+}} &\rightarrow \reals^{\mathcal{T} \cup \mathcal{S^+}} \\
  Df^+(X) &= Df(X) \odot I_{\mathcal{S}^+}.
\end{align*}  

Similarly, if $f \colon \reals^\mathcal{S} \times \reals^\mathcal{T} \rightarrow \reals^\mathcal{U}$, we can extend $f$ to $f^+ \colon \reals^\mathcal{S \cup S^+} \times \reals^\mathcal{T \cup T^+} \rightarrow \reals^\mathcal{U \cup S^+ \cup T^+}$. Then
\begin{align*}
  D_1 f^+(X, Y) &= D_1 f(X, Y) \odot I_{\mathcal{S}^+} \\
  D_2 f^+(X, Y) &= D_2 f(X, Y) \odot I_{\mathcal{T}^+}.
\end{align*}

