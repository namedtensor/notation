\newcommand{\ddx}[1]{\frac{\partial #1}{\partial X}}

Let $f$ be a function from order-$m$ tensors to order-$n$ tensors and let $Y = f(X)$. The partial derivatives of $Y$ with respect to $X$ form an order-$(m+n)$ tensor: $m$ ``input'' axes for the directions in which $X$ could change and $n$ ``output'' axes for the change in $Y$.

For example, if $f$ maps from vectors to vectors, then $\ddx Y$ is a matrix (the Jacobian). But using matrix notation, there are conflicting conventions about whether the first axis is the input axis (``denominator layout'') or the output axis (``numerator layout''). The derivative of a function from vectors to matrices or matrices to vectors cannot be represented as a matrix at all, so one must resort to flattening the matrices into vectors.

With tensors, taking derivatives of higher-order tensors with respect to higher-order tensors is not difficult \citep{laue+:2018}. With named tensors, we get the additional advantage of using names to distinguish input and output axes.

\subsection{Definition}

Let $f \colon \reals^\mathcal{S} \rightarrow \reals^\mathcal{T}$, where $\mathcal{S}$ and $\mathcal{T}$ are orthogonal, and let $Y = f(X)$. Then the derivative of $Y$ at $X$ is the tensor with shape $\mathcal{S} \times \mathcal{T}$ such that for all $s \in \rec\mathcal{S}$ and $t \in \rec\mathcal{T}$,
\begin{equation*}
  \left[\ddx Y \right]_{s,t} = \frac{\partial Y_t}{\partial X_s}.
\end{equation*}

If $X$ and $Y$'s shapes are not orthogonal, we take the derivative of $Y_{\mathcal{T}\rightarrow\mathcal{T'}}$ instead. (It's also possible to rename $X$, but we think it's easier to think about renaming $Y$, so that's what we'll do.) Assume $\mathcal{T} = \ax_1 \times \cdots \times \ax_r$. Then for each $\ax_i$, choose a new name $\ax_i'$ not in either $\mathcal{S}$ or $\mathcal{T}$, and let $\mathcal{T'} = \ax_1' \times \cdots \times \ax_r'$. Then we seek the tensor of partial derivatives
\begin{equation*}
  \left[\ddx{Y_{\mathcal{T}\rightarrow\mathcal{T'}}} \right]_{s,t'} = \frac{\partial Y_t}{\partial X_s}.
\end{equation*}

\subsection{Rules}

To compute derivatives, we use the method of differentials \citep{magnus+neudecker:1985}. The differential of an expression $U$, written $\partial U$, is a tensor with the same shape as $U$, computed using rules like the following:
\begin{align*}
  \partial f(U) &= f'(U) \ndot{\mathcal{U}} \partial U && f \colon \reals^\mathcal{U} \rightarrow \reals^{\mathcal{V}} \\
  \partial (U + V) &= \partial U + \partial V \\
  \partial \nsum{\ax} U &= \nsum{\ax} \partial U \\
  \partial (U \odot V) &= \partial U \odot V + U \odot \partial V \\
  \partial (U \ndot{\ax} V) &= \partial U \ndot{\ax} V + U \ndot{\ax} \partial V \\
  \partial \left(\frac{U}{V}\right) &= \frac{\partial U \odot V - U \odot \partial V}{V^2} \\
  \partial U_s &= \left[\partial U\right]_s \\
  \partial U_{\ax\rightarrow\ax'} &= \left[\partial U\right]_{\ax\rightarrow\ax'}
\end{align*}
If we obtain an equation in the so-called \emph{canonical} form
\begin{equation}
  \partial Y = A \ndot{\mathcal{S}} \partial X + \text{const.}
  \label{eq:canonical}
\end{equation}
where $\mathcal{S}$ is orthogonal to $\mathcal{T}$ and ``const'' stands for terms not depending on $\partial X$, then we have
\begin{equation*}
  \ddx{Y} = A.
\end{equation*}

In order to get equations into canonical form, some tricks are useful. First, contractions can be easier to reason about if rewritten as sums of elementwise products:
\[ A \ndot{\ax} B = \nsum{\ax} A \odot B. \]
Second, renaming can be thought of as contraction with an identity matrix:
\begin{align*}
[I_{\ax,\ax'}]_{\ax(i),\ax'(j)} &= \begin{cases} 1 & i = j \\ 0 & i \neq j \end{cases} \\
A_{\ax\rightarrow\ax'} &= \nsum{\ax} I_{\ax,\ax'} \odot A.
\end{align*}

\subsection{Example}

Let's find the differential of the softmax operator.
\begin{align*}
  Y &= \nfun{\ax}{softmax} X \\
  \partial Y &= \partial \biggl(\frac{\exp X}{\nsum{\ax} \exp X}\biggr) \\
    &= \frac{\exp X \odot \partial X \odot \nsum{\ax} \exp X - \exp X \odot \nsum{\ax} (\exp X \odot \partial X)}{\bigl(\nsum{\ax} \exp X\bigr)^2} \\
  &= Y \odot (\partial X - Y \ndot{\ax} \partial X).
\end{align*}

Next, use this to find the Jacobian, $\ddx Y$. Since $X$ and $Y$ have the same shape, we rename $Y$:
\begin{align*}
  \partial Y_{\ax\rightarrow\ax'} &= [Y \odot (\partial X - \nsum{\ax} Y \odot \partial X)]_{\ax\rightarrow\ax'} \\
  &= Y_{\ax\rightarrow\ax'} \odot (\partial X_{\ax\rightarrow\ax'} - \nsum{\ax} Y \odot \partial X) \\
  &= Y_{\ax\rightarrow\ax'} \odot \left(\nsum{\ax} I_{\ax',\ax} \odot \partial X - \nsum{\ax} Y \odot \partial X\right) \\
  &= \nsum{\ax} Y_{\ax\rightarrow\ax'} \odot (I_{\ax',\ax} - Y) \odot \partial X \\
  \ddx {Y_{\ax\rightarrow\ax'}} &= Y_{\ax\rightarrow\ax'} \odot (I_{\ax',\ax} - Y).
\end{align*}

To derive the rule for backpropagation, we assume a function $f \colon \reals^\ax \rightarrow \reals$ and differentiate $f(Y)$. Since $f$ is scalar-valued, there is no name overlap, so no renaming is needed.
\begin{align*}
  \partial f(Y) &= \nsum{\ax} f'(Y) \odot \partial Y \\
  &= \nsum{\ax} f'(Y) \odot Y \odot (\partial X - \nsum{\ax} Y \odot \partial X) \\
  %&= f'(Y) \ndot{\ax} (Y \odot (\partial X - Y \ndot{\ax} \partial X)) \\
  &= \nsum{\ax} f'(Y) \odot Y \odot \partial X - \nsum{\ax} f'(Y) \odot Y \odot \nsum{\ax} Y \odot \partial X \\
  &= \nsum{\ax} f'(Y) \odot Y \odot \partial X - \nsum{\ax} \left(\nsum{\ax} f'(Y) \odot Y\right) \odot Y \odot \partial X \\
  &= \nsum{\ax} Y \odot (f'(Y) - \nsum{\ax} f'(Y) \odot Y) \odot \partial X \\
  \ddx{f(Y)} &= Y \odot (f'(Y) - f'(Y) \ndot{\ax} Y).
\end{align*}

\subsection{Broadcasting}

Let $f \colon \reals^\mathcal{S} \rightarrow \reals^\mathcal{T}$, and let $f'$ be its derivative. If $X \in \reals^{\mathcal{S} \cup \mathcal{U}}$, where $\mathcal{U}$ is orthogonal to both $\mathcal{S}$ and $\mathcal{T}$, recall that $Y = f(X)$ is defined by:
\begin{align*}
  Y_r &= f(X_r)
\end{align*}
Finding the differential of $Y$ is easy:
\begin{align*}
  \partial Y_r &= f'(X_r) \ndot{\mathcal{S}} \partial X_r \\
  \partial Y &= f'(X) \ndot{\mathcal{S}} \partial X.
\end{align*}
But although $f'$ extends to $X$ using the usual broadcasting rules, it's not the case that $\ddx Y = f'(X)$, which would have the wrong shape. The reason is that the contraction is only over $\mathcal{S}$, not $\mathcal{S}\cup\mathcal{U}$. To get this into the form (\ref{eq:canonical}):
\begin{align*}
  \partial Y_{\mathcal{U}\rightarrow\mathcal{U'}} &= \nsum{\mathcal{S}} [f'(X) \odot \partial X]_{\mathcal{U}\rightarrow\mathcal{U'}} \\
  &= \nsum{\mathcal{S}} \nsum{\mathcal{U}} I_{\mathcal{U},\mathcal{U'}} \odot f'(X) \odot \partial X \\
  \ddx{Y_{\mathcal{U}\rightarrow\mathcal{U'}}} &= I_{\mathcal{U},\mathcal{U'}} \odot f'(X).
\end{align*}
In general, then, when we extend a function to new axes, we extend its derivative by multiplying by the identity matrix for those axes.
