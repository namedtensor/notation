\subsection{Derivatives}

\newcommand{\ddx}{\frac{\partial}{\partial x}}
\newcommand{\ddX}{\frac{\partial}{\partial X}}
\newcommand{\ddy}{\frac{\partial}{\partial y}}

Using matrix notation, taking derivatives with respect to vectors or matrices is tricky. With tensors, it's relatively easy \citep{laue+:2018}, and likewise with named tensors.

\paragraph{Definition}

Let $f \colon \reals^\mathcal{S} \rightarrow \reals^\mathcal{T}$. The derivative of $f$ adds $|\mathcal{S}|$ new axes to the return type of $f$, and they must not already be names in $\mathcal{T}$. So if $\mathcal{S} = \name{ax_1} \times \cdots \times \name{ax\sub{r}}$, then for each axis name $\name{ax\sub{i}}$, let $\name{ax\sub{i}'}$ be a new axis name, not in $\mathcal{T}$, and let $\mathcal{S'} = \name{ax_1'} \times \cdots \times \name{ax\sub{r}'}$. If $s = \{\nidx{ax_1}{i_1}, \ldots, \nidx{ax\sub{r}}{i_r}\}$, let $s' = \{\nidx{ax'_1}{i_1}, \ldots, \nidx{ax'\sub{r}}{i_r}\}$.

Then the derivative of $f$ at $x$ is the tensor with shape $\mathcal{S}' \times \mathcal{T}$ such that for all $s \in \rec\mathcal{S}$ and $t \in \rec\mathcal{T}$,
\[\left[\ddx f(x) \right]_{s',t} = \frac{\partial}{\partial x_{s'}} [f(x)]_t.\]

The primed axis names are inconvenient, but fortunately we usually don't have to work with them explicitly.

\paragraph{Rules}

Below, we give some rules for computing derivatives.
Unless otherwise indicated, $x$ has shape $\mathcal{S}$, and $u$ and $v$ are dependent on $x$ and have shapes $\mathcal{U}$ and $\mathcal{V}$, respectively.
Define $I_\mathcal{S}$, a generalization of the identity matrix:
\begin{align*}
  I_\mathcal{S} &\in \reals^{\mathcal{S} \times \mathcal{S'}} \\
  [I_\mathcal{S}]_{s, s'} &= \begin{cases}
    1 & \text{if $s = s'$} \\
    0 & \text{otherwise.}
  \end{cases}
\end{align*}

\begin{align*}
  \ddx x &= I_\mathcal{S} \\
  \ddx u &= 0 && \text{$u$ does not depend on $x$} \\
  \ddx f(u) &= f'(u) \odot \ddx u && f \colon \reals \rightarrow \reals \\
  \ddx (u + v) &= \ddx u + \ddx v \\
  \ddx \nsum{ax} u &= \nsum{ax} \ddx u \\
  \ddx (u \odot v) &= \ddx u \odot v + u \odot \ddx v \\
  \ddx (u \ndot{ax} v) &= \ddx u \ndot{ax} v + u \ndot{ax} \ddx v \\
  \ddx \frac{u}{v} &= \frac{\ddx u \odot v - u \odot \ddx v}{v^2} \\
  \ddx u_r &= \left[\ddx u\right]_r && r \in \rec \mathcal{R}, \mathcal{R} \subseteq \mathcal{U} \\
  \ddx u_{\nmov{ax1}{ax2}} &= \left[\ddx u\right]_{\nmov{ax1}{ax2}}
\end{align*}

Here's an example using these rules to derive a new one:
\begin{align*}
  \ddx (\nfun{ax}{softmax} u) &= \ddx \frac{\exp u}{\nsum{ax} \exp u} \\
    &= \frac{\exp u \odot \ddx u \odot \nsum{ax} \exp u - \exp u \odot \nsum{ax} (\exp u \odot \ddx u)}{(\nsum{ax} \exp u)^2} \\
    &= \nfun{ax}{softmax} u \odot \left(\ddx u - \nfun{ax}{softmax} u \ndot{ax} \ddx u\right).
\end{align*}

\paragraph{Chain rule and backpropagation}

Above we gave a chain rule for elementwise operations. The general chain rule looks like this for functions of one and two variables; three or more variables are analogous.
\begin{align*}
  \ddx f(u) &= \frac{\partial}{\partial u} f(u) \ndot{\mathcal{U}' \mid \mathcal{U}} \ddx u \\
  \ddx f(u, v) &= \frac{\partial}{\partial u} f(u, v) \ndot{\mathcal{U}' \mid \mathcal{U}} \ddx u + \frac{\partial}{\partial v} f(u, v) \ndot{\mathcal{V}' \mid \mathcal{V}} \ddx v
\end{align*}
where $\ndot{ax1|ax2}$ contracts $\name{ax1}$ in the left operand with $\name{ax2}$ in the right operand: $A \ndot{ax1|ax2} B = \sum_i A_{\nidx{ax1}{i}} \odot B_{\nidx{ax2}{i}}$.

The general chain rule is needed for backpropagation. For example, to derive the backpropagation rule for softmax:
\begin{align*}
  \ddx f(\nfun{ax}{softmax} x) &= f'(\nfun{ax}{softmax} x) \ndot{\mathcal{S'}|\mathcal{S}} \ddx \nfun{ax}{softmax} x \\
    &= f'(\nfun{ax}{softmax} x) \ndot{\mathcal{S'}|\mathcal{S}} \nfun{ax}{softmax} x \odot \left(I_\mathcal{S} - \nfun{ax}{softmax} x \ndot{ax} I_\mathcal{S}\right).
\end{align*}

\paragraph{Broadcasting}

If $f \colon \reals^\mathcal{S} \rightarrow \reals^\mathcal{T}$, then recall that $f$ can be extended to $\reals^{\mathcal{S} \cup \mathcal{S^+}}$ where $\mathcal{S}$ and $\mathcal{S^+}$ are orthogonal.

It's more convenient here to notate the derivative of $f$ as $Df$. If $f$ has two arguments, its partial derivatives are $D_1 f$ and $D_2 f$.

Although $Df$ extends to $\reals^{\mathcal{S} \cup \mathcal{S^+}}$ using the usual broadcasting rules, the extension of the derivative is unfortunately not the derivative of the extension. To avoid confusion, write $f^+$ for the extension:
\begin{align*}
  f^+ \colon \reals^{\mathcal{S} \cup \mathcal{S^+}} &\rightarrow \reals^{\mathcal{T} \cup \mathcal{S^+}} \\
  f^+(x)_r &= f(x_r).
\end{align*}
Then the derivative of $f^+$ is:
\begin{align*}
  Df^+ \colon \reals^{\mathcal{S'} \cup \mathcal{{S^+}'} \cup \mathcal{T} \cup \mathcal{S^+}} &\rightarrow \reals^{\mathcal{T} \cup \mathcal{S^+}} \\
  Df^+(x) &= Df(x) \odot I_{\mathcal{S}^+}.
\end{align*}  

Similarly, if $f \colon \reals^\mathcal{S} \times \reals^\mathcal{T} \rightarrow \reals^\mathcal{U}$, we can extend $f$ to $f^+ \colon \reals^\mathcal{S \cup S^+} \times \reals^\mathcal{T \cup T^+} \rightarrow \reals^\mathcal{U \cup S^+ \cup T^+}$. Then
\begin{align*}
  D_1 f^+(x, y) &= D_1 f(x, y) \odot I_{\mathcal{S}^+} \\
  D_2 f^+(x, y) &= D_2 f(x, y) \odot I_{\mathcal{T}^+}.
\end{align*}

