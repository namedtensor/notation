\subsection{Derivatives}

\newcommand{\ddx}{\frac{\partial}{\partial x}}

If $f$ is a function from order-$m$ tensors to order-$n$ tensors, the partial derivatives of $f$ (evaluated on a tensor $x$) form an order-$(m+n)$ tensor: $m$ ``input'' axes for the directions in which $x$ could change and $n$ ``output'' axes for the change in $f(x)$.

For example, the derivative of a function from vectors to vectors is a matrix (the Jacobian). But using matrix notation, there are conflicting conventions about whether the first axis is the input axis (``denominator layout'') or the output axis (``numerator layout''). The derivative of a function from vectors to matrices or matrices to vectors cannot be represented as a matrix at all, so one must resort to flattening the matrices into vectors.

With tensors, taking derivatives of higher-order tensors with respect to higher-order tensors is not difficult \citep{laue+:2018}. With named tensors, we get the additional advantage of using names to distinguish input and output axes.

\paragraph{Definition}

Let $f \colon \reals^\mathcal{S} \rightarrow \reals^\mathcal{T}$. The derivative of $f$ (evaluated at $x$) has an input axis for each axis in $\mathcal{S}$ and an output axis for each axis in $\mathcal{T}$, and they have to have distinct names. So if $\mathcal{S} = \name{ax_1} \times \cdots \times \name{ax\sub{r}}$, then for each axis name $\name{ax\sub{i}}$, let $\name{ax\sub{i}'}$ be a new axis name, not in $\mathcal{T}$, and let $\mathcal{S'} = \name{ax_1'} \times \cdots \times \name{ax\sub{r}'}$. If $s = \{\nidx{ax_1}{i_1}, \ldots, \nidx{ax\sub{r}}{i_r}\}$, let $s' = \{\nidx{ax'_1}{i_1}, \ldots, \nidx{ax'\sub{r}}{i_r}\}$.

Then the derivative of $f$ at $x$ is the tensor with shape $\mathcal{S}' \times \mathcal{T}$ such that for all $s \in \rec\mathcal{S}$ and $t \in \rec\mathcal{T}$,
\[\left[\ddx f(x) \right]_{s',t} = \frac{\partial}{\partial x_{s'}} [f(x)]_t.\]

We'll often make use of the following generalization of the identity matrix:
\begin{align*}
  I_\mathcal{S} &\in \reals^{\mathcal{S'} \times \mathcal{S}} \\
  [I_\mathcal{S}]_{s', s} &= \begin{cases}
    1 & \text{if $s' = s$} \\
    0 & \text{otherwise.}
  \end{cases}
\end{align*}

\paragraph{Rules}

Now we give some rules for computing derivatives. Unless otherwise indicated, $x$ has shape $\mathcal{S}$, and $u$ and $v$ are dependent on $x$ and have shapes $\mathcal{U}$ and $\mathcal{V}$, respectively.
\begin{align*}
  \ddx x &= I_\mathcal{S} \\
  \ddx u &= 0 && \text{$u$ does not depend on $x$} \\
  \ddx f(u) &= f'(u) \odot \ddx u && f \colon \reals \rightarrow \reals \\
  \ddx (u + v) &= \ddx u + \ddx v \\
  \ddx \nsum{ax} u &= \nsum{ax} \ddx u \\
  \ddx (u \odot v) &= \ddx u \odot v + u \odot \ddx v \\
  \ddx (u \ndot{ax} v) &= \ddx u \ndot{ax} v + u \ndot{ax} \ddx v \\
  \ddx \frac{u}{v} &= \frac{\ddx u \odot v - u \odot \ddx v}{v^2} \\
  \ddx u_r &= \left[\ddx u\right]_r && r \in \rec \mathcal{R}, \mathcal{R} \subseteq \mathcal{U} \\
  \ddx u_{\nmov{ax1}{ax2}} &= \left[\ddx u\right]_{\nmov{ax1}{ax2}}
\end{align*}

Here's an example using these rules to derive a new one:
\begin{align*}
  \ddx (\nfun{ax}{softmax} u) &= \ddx \frac{\exp u}{\nsum{ax} \exp u} \\
    &= \frac{\exp u \odot \ddx u \odot \nsum{ax} \exp u - \exp u \odot \nsum{ax} (\exp u \odot \ddx u)}{(\nsum{ax} \exp u)^2} \\
    &= \nfun{ax}{softmax} u \odot \left(\ddx u - \nfun{ax}{softmax} u \ndot{ax} \ddx u\right).
\end{align*}

\paragraph{Chain rule and backpropagation}

Above, we gave a chain rule for elementwise operations. The general chain rule looks like this for functions of one and two variables; three or more variables are analogous.
\begin{align*}
  \ddx f(u) &= \frac{\partial}{\partial u} f(u) \ndot{\mathcal{U}' \mid \mathcal{U}} \ddx u \\
  \ddx f(u, v) &= \frac{\partial}{\partial u} f(u, v) \ndot{\mathcal{U}' \mid \mathcal{U}} \ddx u + \frac{\partial}{\partial v} f(u, v) \ndot{\mathcal{V}' \mid \mathcal{V}} \ddx v
\end{align*}
where $\ndot{ax1|ax2}$ contracts $\name{ax1}$ in the left operand with $\name{ax2}$ in the right operand: $A \ndot{ax1|ax2} B = \sum_i A_{\nidx{ax1}{i}} \odot B_{\nidx{ax2}{i}}$.

The general chain rule is needed for backpropagation. For example, to derive the backpropagation rule for softmax:
\begin{align*}
  \ddx f(\nfun{ax}{softmax} x) &= f'(\nfun{ax}{softmax} x) \ndot{\mathcal{S'}|\mathcal{S}} \ddx \nfun{ax}{softmax} x \\
    &= f'(\nfun{ax}{softmax} x) \ndot{\mathcal{S'}|\mathcal{S}} \nfun{ax}{softmax} x \odot \left(I_\mathcal{S} - \nfun{ax}{softmax} x \ndot{ax} I_\mathcal{S}\right).
\end{align*}

\paragraph{Broadcasting}

If $f \colon \reals^\mathcal{S} \rightarrow \reals^\mathcal{T}$, then recall that $f$ can be extended to $\reals^{\mathcal{S} \cup \mathcal{S^+}}$ where $\mathcal{S}$ and $\mathcal{S^+}$ are orthogonal.

It's more convenient here to notate the derivative of $f$ as $Df$. If $f$ has two arguments, its partial derivatives are $D_1 f$ and $D_2 f$.

Although $Df$ extends to $\reals^{\mathcal{S} \cup \mathcal{S^+}}$ using the usual broadcasting rules, the extension of the derivative is unfortunately not the derivative of the extension. To avoid confusion, write $f^+$ for the extension:
\begin{align*}
  f^+ \colon \reals^{\mathcal{S} \cup \mathcal{S^+}} &\rightarrow \reals^{\mathcal{T} \cup \mathcal{S^+}} \\
  f^+(x)_r &= f(x_r).
\end{align*}
Then the derivative of $f^+$ is:
\begin{align*}
  Df^+ \colon \reals^{\mathcal{S'} \cup \mathcal{{S^+}'} \cup \mathcal{T} \cup \mathcal{S^+}} &\rightarrow \reals^{\mathcal{T} \cup \mathcal{S^+}} \\
  Df^+(x) &= Df(x) \odot I_{\mathcal{S}^+}.
\end{align*}  

Similarly, if $f \colon \reals^\mathcal{S} \times \reals^\mathcal{T} \rightarrow \reals^\mathcal{U}$, we can extend $f$ to $f^+ \colon \reals^\mathcal{S \cup S^+} \times \reals^\mathcal{T \cup T^+} \rightarrow \reals^\mathcal{U \cup S^+ \cup T^+}$. Then
\begin{align*}
  D_1 f^+(x, y) &= D_1 f(x, y) \odot I_{\mathcal{S}^+} \\
  D_2 f^+(x, y) &= D_2 f(x, y) \odot I_{\mathcal{T}^+}.
\end{align*}

