\subsubsection{Named and numbered axes}
\label{sec:tensorsoftensors}

We allow axes to have names that are natural numbers $1, 2, \ldots$, and we define ``numbering'' and ``naming'' operators:
\begin{center}
\begin{tabular}{cl}
$A_{\name{ax}}$ & rename axis $\name{ax}$ to 1 \\
$A_{\name{ax1},\name{ax2}}$ & rename axis $\name{ax1}$ to 1 and $\name{ax2}$ to 2 \\
$A_{\rightarrow\name{ax}}$ & rename axis 1 to $\name{ax}$ \\
$A_{\rightarrow\name{ax1},\name{ax2}}$ & rename axis 1 to $\name{ax1}$ and 2 to $\name{ax2}$
\end{tabular}
\end{center}
The numbering operators are only defined on tensors that have no numbered axes.

Then we adopt the convention that standard vector/matrix operations operate on the numbered axes. For example, vector dot-product always uses axis 1 of both its operands, so that we can write
\begin{equation*}
C = A_{\name{ax}} \cdot B_{\name{ax}}
\end{equation*}
equivalent to $C = A \ndot{ax} B$. 

Previously, we had to define a new version of every operation; most of the time, it looked similar to the standard version (e.g., $\max$ vs $\max_{\name{ax}}$), but occasionally it looked quite different (e.g., matrix inversion). With numbered axes, we can use standard notation for everything.
(This also suggests a clean way to integrate code that uses named tensors with code that uses ordinary tensors.)

We also get the renaming operation for free: $A_{\name{ax1}\rightarrow\name{ax2}} = [A_{\name{ax1}}]_{\rightarrow\name{ax2}}$ renames axis $\name{ax1}$ to $\name{ax2}$.

Finally, this notation alleviates the duality problem, as can be seen in the definition of a RNN:
\begin{align*}
x^{t} &\in \mathbb{R}^{\name{input}} & t &= 1, \ldots, n \\
W^{\text{h}} &\in \mathbb{R}^{\name{hidden} \times \name{hidden'}} & |\name{hidden}| &= |\name{hidden'}| \\
W^{\text{i}} &\in \mathbb{R}^{\name{input} \times \name{hidden}} \\
b &\in \mathbb{R}^{\name{hidden}} \\
h^{0} &\in \mathbb{R}^{\name{hidden}} \\
h^{t} &= \sigma\left( W^{\text{h}}_{\name{hidden'}} \cdot h^{t-1}_{\name{hidden}} + W^{\text{i}}_{\name{input}} \cdot x^{t}_{\name{input}} + b_{\name{hidden}} \right) & t &= 1, \ldots, n
\end{align*}

Attention requires either a renaming or a transpose:
\begin{align*}
  \text{Att} \colon \mathbb{R}^{\name{key}} \times \mathbb{R}^{\name{seq} \times\name{key}} \times \mathbb{R}^{\name{seq} \times\name{val}} &\rightarrow \mathbb{R}^{\name{val}} \\
  \text{Att}(Q,K,V) &= \softmax \left[ \frac{Q_{\name{key}} \cdot \nmov{seq}{seq'}{K_{\name{key}}}}{\sqrt{|\name{key}|}} \right]_{\name{seq'}} \cdot V_{\name{seq}} \\
  |\name{seq}| &= |\name{seq'}| \\
  Q &= W^{l,Q} \ndot{layer} X & W^{l,Q} &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{key}} \\
  K &= W^{l,K} \ndot{layer} X & W^{l,K} &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{key}} \\
  V &= W^{l,V} \ndot{layer} X & W^{l,V} &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{val}} \\
  Y &= \nsum{heads} W^{l,O} \ndot{val} \text{Att}(Q, K, V, M) & W^{l,O} &\in \mathbb{R}^{\name{heads} \times \name{val} \times \name{layer}}
\end{align*}

The multivariate normal distribution also needs a renaming or transpose:
\begin{align*} 
X &\in \mathbb{R}^{\name{d}}  \\
\mu &\in \mathbb{R}^{\name{d}}  \\
\Sigma & \in \mathbb{R}^{\name{d1} \times \name{d2}}  \\
{\cal N}(X; \mu, \Sigma) &= \frac{\displaystyle \exp\left(-\tfrac{1}{2} [X - \mu]_{\name{d}}^\top \, \Sigma_{\name{d1},\name{d2}}^{-1} \, [X - \mu]_{\name{d}} \right)}{\sqrt{(2 \pi)^k \, \mathop{\text{det}} \Sigma_{\name{d1},\name{d2}}}}
\end{align*}

Because this notation can be a little more verbose (often requiring you to write axis names twice), we'd keep around the notation $A \ndot{ax} B$ as a shorthand for $A_{\name{ax}} \cdot B_{\name{ax}}$.
