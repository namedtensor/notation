\subsection{Duality}
\label{sec:duality}

In applied linear algebra, we distinguish between column and row vectors; in pure linear algebra, vector spaces and dual vector spaces; in tensor algebra, contravariant and covariant indices; in quantum mechanics, bras and kets. Do we need something like this?

In \S\ref{sec:rnn} we saw that defining an RNN requires renaming of indices, because a linear transformation must map one index to another index; if we want to map an index to itself, we need to use renaming.

In this section, we describe three possible solutions to this problem, and welcome comments about which (if any) would be best.

\subsubsection{Contracting two names}

We define a version of the contraction operator that can contract two indices with different names (and the same size):
\begin{align*}
\nndot{i}{j} &: F^{\nset{i}{n}} \times F^{\nset{j}{n}} \rightarrow F \\
A \nndot{i}{j} B &= \sum_{k=1}^n A_{\nidx{i}{k}} B_{\nidx{j}{k}}.
\end{align*}

For example, the RNN would look like this.
\begin{align*}
x^{(t)} &\in \mathbb{R}^{\nset{emb}{d}} \\
h^{(t)} &\in \mathbb{R}^{\nset{state}{d}} \\
A &\in \mathbb{R}^{\nset{state}{d} \times \nset{state'}{d}} \\
B &\in \mathbb{R}^{\nset{emb}{d} \times \nset{state}{d}} \\
c &\in \mathbb{R}^{\nset{state}{d}} \\
h^{(t+1)} &= \tanh\left( A \nndot{state'}{state} h^{(t)} + B \ndot{emb} x^{(t)} + c \right)
\end{align*}

\subsubsection{Starred index names}

If $\name{i}$ is a name, we also allow a tensor to have an index $\name{i*}$ (alternatively: superscript $\name{i}$). Multiplication contracts starred indices in the left operand with non-starred indices in the right operand.
\begin{align*}
x^{(t)} &\in \mathbb{R}^{\nset{emb}{d}} \\
h^{(t)} &\in \mathbb{R}^{\nset{state}{d}} \\
A &\in \mathbb{R}^{\nset{state*}{d} \times \nset{state}{d}} \\
B &\in \mathbb{R}^{\nset{emb*}{d} \times \nset{state}{d}} \\
c &\in \mathbb{R}^{\nset{state}{d}} \\
h^{(t+1)} &= \tanh\left( A \ndot{state} h^{(t)} + B \ndot{emb} x^{(t)} + c \right) 
\end{align*}
The contraction operator can be defined as:
\begin{align*}
\ndot{i} &: F^{\nset{i*}{n}} \times F^{\nset{i}{n}} \rightarrow F \\
A \ndot{i} B &= \sum_{i=1}^n A_{\nidx{i*}{i}} B_{\nidx{i}{i}}.
\end{align*}

There are a few variants of this idea that have been floated:
\begin{enumerate}
\item $\ndot{}$ (no subscript) contracts every starred index in its left operand with every corresponding unstarred index in its right operand. Rejected.
\item $\ndot{i}$ contracts $\name{i}$ with $\name{i}$, and we need another notation like $\ndot{i(*)}$ or $\nbin{i}{\times}$ for contracting $\name{i*}$ with $\name{i}$.
\item $\ndot{i}$ always contracts $\name{i*}$ with $\name{i}$; there's no way to contract $\name{i}$ with $\name{i}$.
\end{enumerate}

\subsubsection{Named and numbered indices}
\label{sec:tensorsoftensors}

We allow indices to have names that are natural numbers $1, 2, \ldots$, and we define ``numbering'' and ``naming'' operators:
\begin{center}
\begin{tabular}{cl}
$A_{\name{i}}$ & rename index $\name{i}$ to 1 \\
$A_{\name{i},\name{j}}$ & rename index $\name{i}$ to 1 and $\name{j}$ to 2 \\
$A_{\rightarrow\name{i}}$ & rename index 1 to $\name{i}$ \\
$A_{\rightarrow\name{i},\name{j}}$ & rename index 1 to $\name{i}$ and 2 to $\name{j}$
\end{tabular}
\end{center}
The numbering operators are only defined on tensors that have no numbered indices.

Then we adopt the convention that standard vector/matrix operations operate on the numbered indices. For example, vector dot-product always uses index 1 of both its operands, so that we can write
\begin{equation*}
C = A_{\name{i}} \cdot B_{\name{i}}
\end{equation*}
equivalent to $C = A \ndot{i} B$. 

Previously, we had to define a new version of every operation; most of the time, it looked similar to the standard version (e.g., $\max$ vs $\max_{\name{i}}$), but occasionally it looked quite different (e.g., matrix inversion). With numbered indices, we can use standard notation for everything.
(This also suggests a clean way to integrate code that uses named tensors with code that uses ordinary tensors.)

We also get the renaming operation for free: $A_{\name{i}\rightarrow\name{j}} = [A_{\name{i}}]_{\rightarrow\name{j}}$ renames index $\name{i}$ to $\name{j}$.

Finally, this notation alleviates the duality problem, as can be seen in the definition of a RNN:
\begin{align*}
x^{(t)} &\in \mathbb{R}^{\nset{emb}{d}} \\
h^{(t)} &\in \mathbb{R}^{\nset{state}{d}} \\
A &\in \mathbb{R}^{\nset{state}{d} \times \nset{state'}{d}} \\
B &\in \mathbb{R}^{\nset{state}{d} \times \nset{emb}{d}} \\
c &\in \mathbb{R}^{\nset{state}{d}} \\
h^{(t+1)}_{\name{state}} &= \tanh\left( A_{\name{state},\name{state'}} \, h^{(t)}_{\name{state}} + B_{\name{state},\name{emb}} \, x^{(t)}_{\name{emb}} + c_{\name{state}} \right)
\end{align*}
or equivalently,
\begin{equation*}
h^{(t+1)} = \tanh\left( A_{\name{state'}} \cdot h^{(t)}_{\name{state}} + B_{\name{emb}} \cdot x^{(t)}_{\name{emb}} + c \right)
\end{equation*}

Attention:
\begin{align*}
  \text{Att} &\colon \mathbb{R}^{\nset{seq'}{n'} \times\nset{key}{d_k}} \times \mathbb{R}^{\nset{seq}{n} \times\nset{key}{d_k}} \times \mathbb{R}^{\nset{seq}{n} \times\nset{val}{d_v}} \rightarrow \mathbb{R}^{\nset{seq'}{n'} \times\nset{val}{d_v}} \\
  \text{Att}(Q,K,V) &= \softmax \left[ \frac{Q_{\name{key}} \cdot K_\name{key}}{\sqrt{d_k}} \right]_{\name{seq}} \cdot V_{\name{seq}}
\end{align*}

Multivariate normal distribution:
\begin{align*} 
X &\in \mathbb{R}^{\nset{batch}{{b}} \times \nset{d}{{k}}}  \\
\mu &\in \mathbb{R}^{{\nset{d}{{k}}}}  \\
\Sigma & \in   \mathbb{R}^{{\nset{d}{k} \times \nset{d'}{k}}}  \\
{\cal N}(X; \mu, \Sigma) &= \frac{\displaystyle \exp\left(-\tfrac{1}{2} [X - \mu]_{\name{d}}^\top \, \Sigma_{\name{d},\name{d'}}^{-1} \, [X - \mu]_{\name{d}} \right)}{\sqrt{(2 \pi)^k \, \mathop{\text{det}} \Sigma_{\name{d},\name{d'}}}}
\end{align*}

Because this notation can be a little more verbose (often requiring you to write index names twice), we'd keep around the notation $A \ndot{i} B$ as a shorthand for $A_{\name{i}} \cdot B_{\name{i}}$. We'd also keep named reductions, or at least $\nfun{i}{softmax}$.

