\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{parskip}
\usepackage{natbib}
\usepackage[hidelinks]{hyperref}

\usepackage{namedtensor}

% for formal definitions
\DeclareMathOperator{\ind}{ind}
\DeclareMathOperator{\rec}{rec}
\newcommand{\restrict}[2]{\mathopen{}\left.#1\right|_{#2}}
\newcommand{\nmatrix}[3]{\name{#1}\begin{array}[b]{@{}c@{}}\name{#2}\\\begin{bmatrix}#3\end{bmatrix}\end{array}}

% ugly hack to get non-sans-serif into names
\newcommand{\sub}[1]{_\text{$#1$}}

\allowdisplaybreaks
\DeclareMathOperator*{\softmax}{softmax}
\newcommand{\reals}{\mathbb{R}}

\title{\bf Named Tensor Notation}
\author{David Chiang \\ \small University of Notre Dame \and Sasha Rush \\ \small Cornell University \and Boaz Barak \\ \small Harvard University}
\date{Version 0.3}

\begin{document}

\maketitle

\begin{abstract}
We propose a notation for tensors with named axes, which relieves the author, reader, and future implementers from the burden of keeping track of the order of axes and the purpose of each. It also makes it easy to extend operations on low-order tensors to higher order ones (e.g., to extend an operation on images to minibatches of images, or extend the attention mechanism to multiple attention heads).

After a brief overview of our notation, we illustrate it through several examples from modern machine learning, from building blocks like attention and convolution to full models like Transformers and LeNet. Finally, we give formal definitions and describe some extensions. Our proposals build on ideas from many previous papers and software libraries. We hope that this document will encourage more authors to use named tensors, resulting in clearer papers and less bug-prone implementations.

The source code for this document can be found at \url{https://github.com/namedtensor/notation/}. We invite anyone to make comments on this proposal by submitting issues or pull requests on this repository.
\end{abstract}

\setcounter{tocdepth}{1}
\tableofcontents

\clearpage

\section{Introduction}
\label{sec:intro}

Most papers about neural networks use the notation of vectors and matrices from applied linear algebra. This notation is optimized for talking about vector spaces, but becomes cumbersome when talking about neural networks. Consider the following equation \citep{vaswani+:2017}:
\[ \text{Attention}(Q, K, V) = \softmax \left( \frac{QK^\top}{\sqrt{d_k}} \right) V. \]
where $Q$, $K$, and $V$ (for query, key, and value, respectively) are sequences of feature vectors, packed into matrices. Does the product $QK^\top$ sum over the sequence, or over the features? It sums over columns, but there's not enough information to know what the columns represent. Is the softmax taken over the query sequence or the key sequence? The usual notation doesn't even offer a way to answer this question. With multiple attention heads or multiple sentences in a minibatch, the notation becomes more difficult still.

Here, we propose mathematical notation for tensors with \emph{named axes}. The notation has a formal underpinning, but is hopefully intuitive enough that machine learning researchers can understand it without much effort.

In our notation, the above equation becomes
\begin{align*}
  \text{Attention} \colon \mathbb{R}^{\name{seq'} \times \name{key}} \times \mathbb{R}^{\name{seq} \times\name{key}} \times \mathbb{R}^{\name{seq} \times\name{val}} &\rightarrow \mathbb{R}^{\name{seq'} \times \name{val}} \\
  \text{Attention}(Q,K,V) = \nfun{seq}{softmax} \left( \frac{Q \ndot{key} K}{\sqrt{|\name{key}|}} \right) \ndot{seq} V.
\end{align*}
The tensor $K$ has axes for the sequence (\name{seq}) and for the key features (\name{key}), instead of rows or columns, so the reader does not need to remember which is which. The dot product $Q \ndot{key} K$ is explicitly over the \name{key} axis. The resulting tensor has a \name{seq} axis for the key sequence and a \name{seq'} axis for the query sequence, and the softmax is explicitly over $\name{seq}$, as is the dot product with~$V$.
This formula works as written if we add a $\name{heads}$ axis for multiple attention heads, or a $\name{batch}$ axis for multiple sequences in a minibatch.

Our notation is inspired by libraries for programming with multidimensional arrays \citep{numpy,pytorch} and extensions that use named axes, like xarray \citep{xarray}, Nexus \citep{chen2017typesafe}, tsalib \citep{tsalib}, NamedTensor \citep{namedtensor}, named tensors in PyTorch \citep{named-tensors}, and Dex \citep{maclaurin+:2019}. However, our focus is on mathematical notation rather than code.

The source code for this document can be found at \url{https://github.com/namedtensor/notation/}. We invite anyone to make comments on this proposal by submitting issues or pull requests on this repository.

\section{Informal Overview}
\label{sec:overview}

In standard notation, a vector, matrix, or tensor is indexed by an integer or sequence of integers. If $A \in \reals^{3\times3}$, then the order of the two axes matters: $A_{1,3}$ and $A_{3,1}$ are not the same element. It's up to the reader to remember what each axis of each tensor is for. We think this is a problem and propose a solution.

\subsection{Named tensors}

In a \emph{named tensor}, we give each axis a name. For example, if $A$ represents an image, we can make it a named tensor like so (writing it two equivalent ways to show that the order of axes does not matter):
\begin{align*}
  A &\in \reals^{\nset{height}{3} \times \nset{width}{3}} = \reals^{\nset{width}{3} \times \nset{height}{3}} \\
  A &= \nmatrix{height}{width}{
    3 & 1 & 4 \\
    1 & 5 & 9 \\
    2 & 6 & 5
  } = \nmatrix{width}{height}{
    3 & 1 & 2 \\
    1 & 5 & 6 \\
    4 & 9 & 5
  }.
\end{align*}

We access elements of $A$ using named indices, whose order again does not matter: $A_{\nidx{height}{1}, \nidx{width}{3}} = A_{\nidx{width}{3}, \nidx{height}{1}} = 4$.
We also allow partial indexing:
\begin{align*}
A_{\nidx{height}{1}} &= \nmatrix{}{width}{
  3 & 1 & 4
}
&
A_{\nidx{width}{3}} &= \nmatrix{}{height}{
  4 & 9 & 5
}.
\end{align*}

In many contexts, an axis name is used with only one size. If so, we can simply write $\name{height}$ for the unique axis with name $\name{height}$, as in $\mathbb{R}^{\name{height} \times \name{width}}$. We can leave the size of an axis unspecified at first, and specify its size later (like in a section on experimental details): for example, $|\name{height}|=|\name{width}|=28$ to specify its exact size or just $|\name{height}|=|\name{width}|$ to specify that it's a square image.

What are good choices for axis names? We recommend meaningful \emph{words} instead of single letters, and we recommend words that describe a \emph{whole} rather than its parts. For example, if we wanted $A$ to have red, green, and blue channels, we'd name the axis \name{chans}, and if we wanted to represent a minibatch of images, we'd name the axis \name{batch}. Please see \S\ref{sec:examples} for more examples.

\subsection{Named tensor operations}
\label{sec:operations}

Operations on named tensors are defined by taking a function on low-order tensors and extending it to higher-order tensors.

\subsubsection{Elementwise operations and broadcasting}

Any function from a scalar to a scalar can be applied elementwise to a named tensor, and any function from two scalars to a scalar can be applied to two named tensors with the same shape. For example:
\begin{equation*}
\frac1{1+\exp(-A)} = \nmatrix{height}{width}{
  \frac1{1+\exp(-3)} & \frac1{1+\exp(-1)} & \frac1{1+\exp(-4)} \\[1ex]
  \frac1{1+\exp(-1)} & \frac1{1+\exp(-5)} & \frac1{1+\exp(-9)} \\[1ex]
  \frac1{1+\exp(-2)} & \frac1{1+\exp(-6)} & \frac1{1+\exp(-5)}
}.
\end{equation*}

But if we apply a binary function/operator to tensors with different shapes, they are \emph{broadcast} against each other (similarly to NumPy and derivatives). Let
\begin{align*}
  B &\in \reals^{\nset{height}{3}} & C &\in \reals^{\nset{width}{3}} \\
  B &= \nmatrix{height}{}{
    2 \\ 7 \\ 1
  } & 
  C &= \nmatrix{}{width}{
    1 & 4 & 1
  }.
\end{align*}
(We write $B$ as a column just to make the broadcasting easier to visualize.) Then, to evaluate $A+B$, we effectively replace $B$ with a new tensor $B'$ that contains a copy of $B$ for every index of axis $\name{width}$. Likewise for $A+C$:
\begin{align*}
A + B &= \nmatrix{height}{width}{
  3+2 & 1+2 & 4+2 \\
  1+7 & 5+7 & 9+7 \\
  2+1 & 6+1 & 5+1
} &
A + C &= \nmatrix{height}{width}{
  3+1 & 1+4 & 4+1 \\
  1+1 & 5+4 & 9+1 \\
  2+1 & 6+4 & 5+1
}.
\end{align*}

\subsubsection{Reductions}
\label{sec:reductions}

The same broadcasting rules apply to functions from vectors to scalars, called \emph{reductions}. Unlike with functions on scalars, we always have to specify which axis reductions apply to, using a subscript. (This is equivalent to the \verb|axis| argument in NumPy and \verb|dim| in PyTorch.)

For example, we can sum over the $\name{height}$ axis or the $\name{width}$ axis of $A$:
\begin{align*}
\nsum{height} A &= \sum_i A_{\nidx{height}{i}} = \nmatrix{}{width}{
  3+1+2 & 1+5+6 & 4+9+5
}
\\
\nsum{width} A &= \sum_j A_{\nidx{width}{j}} = \nmatrix{}{height}{
  3+1+4 & 1+5+9 & 2+6+5
}.
\end{align*}
See~\S\ref{sec:reductions} for more examples of reductions.

We can also write multiple names to perform the reduction over multiple axes at once. For example,
\begin{equation*}
  \nsum{height,width} A = \sum_i \sum_j A_{\nidx{height}{i},\nidx{width}{j}} = 3+1+4+1+5+9+2+6+5.
\end{equation*}
%But unlike in NumPy and its derivatives, a reduction with no axis under it is performed over zero axes (which in most cases does nothing).

The vector dot-product is a function from \emph{two} vectors to a scalar, which generalizes to named tensors to give the ubiquitous \emph{contraction} operator. You can think of it as elementwise multiplication, then summation over one axis:
\begin{align*}
%A \ndot{height} B &= \sum_i A_{\nidx{height}{i}} B_{\nidx{height}{i}} = \nmatrix{}{width}{
%  3\cdot2 + 1\cdot7 + 2\cdot1 & 1\cdot2 + 5\cdot7 + 6\cdot1 & 4\cdot2 + 9\cdot7 + 5\cdot 1
%} \\
A \ndot{width} C &= \sum_j A_{\nidx{width}{j}} B_{\nidx{width}{j}} = \nmatrix{height}{}{
  3\cdot1 + 1\cdot4 + 4\cdot1 \\
  1\cdot1 + 5\cdot4 + 9\cdot1 \\
  2\cdot1 + 6\cdot4 + 5\cdot1
}.
\end{align*}
See \S\ref{sec:contractions} for more examples of what the contraction operator can do.

Again, we can write multiple names to contract multiple axes at once. An operator $\odot$ with no axis name under it contracts zero axes and is equivalent to elementwise multiplication, so we use $\odot$ for elementwise multiplication as well.

\subsubsection{Renaming and reshaping}

It's often useful to rename an axis (analogous to a transpose operation in standard notation):
\begin{equation*}
A_{\nmov{height}{height'}} = \nmatrix{height'}{width}{
  3 & 1 & 4 \\
  1 & 5 & 9 \\
  2 & 6 & 5 \\
}.
\end{equation*}
We can also reshape two or more axes into one axis:
\begin{equation*}
  A_{\nmov{(height,width)}{layer}} = \nmatrix{}{layer}{
    3 & 1 & 4 & 1 & 5 & 9 & 2 & 6 & 5
  }
\end{equation*}
%Similarly, we can reshape one axis into two or more axes, or even multiple axes into multiple axes.
The order of elements in the new axis is undefined. If you need a particular order, you can write a more specific definition.

\section{Examples}
\label{sec:examples}

In this section we give a series of examples illustrating how to use named tensors in various situations, mostly related to machine learning.
\subsection{More tensor operations}

\subsubsection{Reductions}
\label{sec:reductions}

\begin{align*}
  \nfun{ax}{min} A &= \min \{A_{\nidx{ax}{i}} \mid 1 \leq i \leq n\} \\
  \nfun{ax}{max} A &= \max \{A_{\nidx{ax}{i}} \mid 1 \leq i \leq n\} \\
  \nfun{ax}{norm} A &= \sqrt{\nsum{ax} A^2} \\
  \nfun{ax}{mean} A &= \frac{1}{n} \nsum{ax} A \\
  \nfun{ax}{var} A &= \frac{1}{n} \nsum{ax} (A - \nfun{ax}{mean} A)^2.
\end{align*}
The $\min$ and $\max$ operators are overloaded, as is the summation operator defined above (\S\ref{sec:reductions}). If the operator is applied to a tensor and has an axis under it, then it's a reduction performed over the axis. But if it is applied to a set of tensors and has no axis under it, then it's an elementwise operation performed over the set.

\subsubsection{Contractions}
\label{sec:contractions}

The contraction operator can be used for many multiplication-like operations.

\begin{align*}
  u, v &\in \mathbb{R}^{\name{ax1}} \\
  x, y &\in \mathbb{R}^{\name{ax2}} \\
  A &\in \mathbb{R}^{\name{ax1} \times \name{ax2}} \\
  B &\in \mathbb{R}^{\name{ax2} \times \name{ax3}}
\end{align*}

\begin{align*}
  u \ndot{ax1} v &= \sum_i u_{\nidx{ax1}{i}} v_{\nidx{ax1}{i}} && \text{inner product} \\
  u \odot x &= \sum_{i,j} u_{\nidx{ax1}{i}} x_{\nidx{ax2}{j}} && \text{outer product} \\
  A \ndot{ax2} x &= \sum_j A_{\nidx{ax2}{j}} x_{\nidx{ax2}{j}} && \text{matrix-vector multiplication} \\
  u \ndot{ax1} A &= \sum_i u_{\nidx{ax1}{j}} A_{\nidx{ax1}{j}} && \text{vector-matrix multiplication} \\
  A \ndot{ax2} B &= \sum_j A_{\nidx{ax2}{j}} \odot B_{\nidx{ax2}{j}} && \text{matrix-matrix multiplication}
\end{align*}

\subsubsection{Softmax and argmax}

Most activation functions are elementwise operations (sigmoid, tanh, ReLU), so they are straightforward to use in our notation; the softmax, however, is interesting because it's defined as a function from vectors to vectors:
\begin{align*}
  \nfun{ax}{softmax} A &= \frac{\exp A}{\nsum{ax} \exp A}.
\end{align*}
As with reductions, we write an axis below the softmax operator, but this axis is retained in the output.

Closely related are argmax and argmin, which we define to compute one-hot vectors with a one at the position containing the maximum or minimum value.
\begin{align*}
  \nfun{ax}{argmax} A &= \lim_{\alpha \rightarrow \infty} \nfun{ax}{softmax} \alpha A \\
  \nfun{ax}{argmin} A &= \lim_{\alpha \rightarrow -\infty} \nfun{ax}{softmax} \alpha A.
\end{align*}

\subsection{Building blocks}

\subsubsection{Fully-connected layers}

A feedforward neural network looks like this:
\begin{align*}
  X^0 &\in \mathbb{R}^{\name{input}} \\
  X^1 &= \sigma(W^1 \ndot{input} X^0 + b^1) & W^1 &\in \mathbb{R}^{\name{hidden1} \times \name{input}} & b^1 &\in \mathbb{R}^{\name{hidden1}} \\
  X^2 &= \sigma(W^2 \ndot{hidden1} X^1 + b^2) & W^2 &\in \mathbb{R}^{\name{hidden2} \times \name{hidden1}} & b^2 &\in \mathbb{R}^{\name{hidden2}} \\
  X^3 &= \sigma(W^3 \ndot{hidden2} X^2 + b^3) & W^3 &\in \mathbb{R}^{\name{output} \times \name{hidden2}} & b^3 &\in \mathbb{R}^{\name{output}}
\end{align*}
The layer sizes can be set by writing $|\name{input}| = 100$, etc.

If you don't like repeating the equations for fully-connected layers, you can put them inside a function:
\begin{align*}
  \text{FullConn}^l(x) &= \sigma\left(W^l \ndot{layer} x + b^l\right)_{\nmov{layer'}{layer}}
\end{align*}
where
\begin{align*}
  W^l &\in \mathbb{R}^{\nset{layer'}{n_{l}} \times \nset{layer}{n_{l-1}}} \\
  b^l &\in \mathbb{R}^{\nset{layer'}{n_l}}.
\end{align*}
A couple of things are new here. First, $\text{FullConn}^l$ encapsulates both the equation for layer $l$ as well as its parameters (analogous to what TensorFlow and PyTorch call \emph{modules}). Second, we chose to use the same axis name \name{layer} for all the layers (with different sizes $n_l$). So $\text{FullConn}^l$ temporarily computes its output over axis \name{layer'}, then renames it back to \name{layer}. 

Then the network can be defined like this:
\begin{align*}
  X^0 &\in \mathbb{R}^{\nset{layer}{n_0}} \\
  X^1 &= \text{FullConn}^1(X^0) \\
  X^2 &= \text{FullConn}^2(X^1) \\
  X^3 &= \text{FullConn}^3(X^2).
\end{align*}

\subsubsection{Recurrent neural networks}
\label{sec:rnn}

As a second example, let's define a simple (Elman) RNN. This is similar to the feedforward network, except that the number of timesteps is variable and they all share parameters.
\begin{align*}
x^{t} &\in \mathbb{R}^{\name{input}} & t &= 1, \ldots, n \\
W^{\text{h}} &\in \mathbb{R}^{\name{hidden} \times \name{hidden'}} & |\name{hidden}| &= |\name{hidden'}| \\
W^{\text{i}} &\in \mathbb{R}^{\name{input} \times \name{hidden'}} \\
b &\in \mathbb{R}^{\name{hidden'}} \\
h^{0} &\in \mathbb{R}^{\name{hidden}} \\
h^{t} &= \sigma\left( W^{\text{h}} \ndot{hidden} h^{t-1} + W^{\text{i}} \ndot{input} x^{t} + b \right)_{\nmov{hidden'}{hidden}} & t &= 1, \ldots, n
\end{align*}

\subsubsection{Attention}
\label{sec:attention}

In the introduction (\S\ref{sec:intro}), we mentioned some difficulties in interpreting the equation for attention as it's usually written. In our notation, it looks like this:
\begin{align*}
  \text{Attention} \colon \mathbb{R}^{\name{key}} \times \mathbb{R}^{\name{seq} \times\name{key}} \times \mathbb{R}^{\name{seq} \times\name{val}} &\rightarrow \mathbb{R}^{\name{val}} \\
  \text{Attention}(Q,K,V) &= \nfun{seq}{softmax} \left( \frac{Q \ndot{key} K}{\sqrt{|\name{key}|}} \right) \ndot{seq} V.
\end{align*}

This equation is slightly different from the one in the introduction. The previous definition computed an output sequence over axis \name{seq'}, but this definition computes a single value. If we want a sequence, we can just give $Q$ a \name{seq'} axis (or some other name), and the function will compute an output sequence. Furthermore, if we give $Q$, $K$, and $V$ a \name{heads} axis for multiple attention heads, then the function will compute multi-head attention.

Sometimes we need to apply a mask to keep from attending to certain positions.
\begin{align*}
  \text{Attention} \colon \mathbb{R}^{\name{key}} \times \mathbb{R}^{\name{seq} \times\name{key}} \times \mathbb{R}^{\name{seq} \times\name{val}} \times \mathbb{R}^{\name{seq}} &\rightarrow \mathbb{R}^{\name{val}} \\
\text{Attention}(Q, K, V, M) &= \nfun{seq}{softmax} \left( \frac{Q \ndot{key} K}{\sqrt{|\name{key}|}} + M \right) \ndot{seq} V.
\end{align*}

\subsubsection{Convolution}

A 1-dimensional convolution can be written using contractions:
\begin{align*}
\text{Conv1d} \colon \reals^{\name{chans} \times \nset{seq}{n}} &\rightarrow \mathbb{R}^{\nset{seq}{n'}} \\
\text{Conv1d}(X; W, b) &= [W \ndot{chans,kernel} C \ndot{seq} X + b]_{\nmov{out}{seq}}
\end{align*}
where
\begin{align*}
W &\in \reals^{\name{chans} \times \name{kernel}} \\
b &\in \reals \\
n' &= n-|\name{kernel}|+1 \\
C &\in \reals^{\nset{out}{n'} \times \name{kernel} \times \nset{seq}{n}} \\
C_{\nidx{out}{o},\nidx{kernel}{k},\nidx{seq}{i}} &= \delta(o+k-1,i).
\end{align*}
This computes a single output channel, but we can get multiple output channels by giving $W$ and $b$ a \name{chans'} axis (or some other name).

We can use the same $C$ to define a 2-dimensional convolution:
\begin{align*}
  \text{Conv2d} \colon \reals^{\name{chans} \times \nset{height}{h} \times \nset{width}{w}}
  &\rightarrow \reals^{\nset{height}{h'} \times \nset{width}{w'}} \\
  \text{Conv2d}(X; W, b) &= [W \ndot{chans, kh, kw} C^2 \ndot{height,width} X + b]_{\substack{\nmov{oh}{height}\\\nmov{ow}{width}}}
\end{align*}  
where
\begin{align*}
W &\in \reals^{\name{chans} \times \name{kh} \times \name{kw}} \\
b &\in \reals \\
h' &= h-|\name{kh}|+1 \\
w' &= w-|\name{kw}|+1 \\
C^2 &= C_{\substack{\nmov{out}{oh} \\ \nmov{kernel}{kh} \\ \nmov{seq}{height}}} \odot C_{\substack{\nmov{out}{ow} \\ \nmov{kernel}{kw} \\ \nmov{seq}{width}}}.
\end{align*}

\subsubsection{Max pooling}

\begin{align*}
\text{MaxPool1d}_{k} \colon \mathbb{R}^{\nset{seq}{n}} &\rightarrow \mathbb{R}^{\nset{seq}{n/k}} \\
\text{MaxPool1d}_{k}(X) &= \nfun{k}{max} U
\end{align*}
where
\begin{align*}
U &\in \reals^{\nset{seq}{n / k} \times \nset{k}{k}} \\
U_{\nidx{seq}{i}, \nidx{k}{di}} & = X_{\nidx{seq}{i \times k + di -1}}.
\end{align*}


\begin{align*}
\text{MaxPool2d}_{kh,kw} \colon \mathbb{R}^{\nset{height}{h} \times \nset{width}{w}} &\rightarrow \mathbb{R}^{\nset{height}{h/kh} \times \nset{width}{w/kw}} \\
\text{MaxPool2d}_{kh,hw}(X) &= \nfun{kh, kw}{max} U
\end{align*}
where
\begin{align*}
U &\in \reals^{\nset{height}{h / kh} \times \nset{width}{w / kw} \times \nset{kh}{kh} \times \nset{kw}{kw}} \\
U_{\nidx{height}{i}, \nidx{width}{j}, \nidx{kh}{di}, \nidx{kw}{dj}} & = X_{\nidx{height}{i \times kh + di -1}, \nidx{width}{j \times kw + dj -1}}.
\end{align*}

\subsubsection{Normalization layers}

Batch, instance, and layer normalization are often informally described using the same
equation, but they each correspond to very different functions. They differ
by which axes are normalized.

We can define a single generic normalization layer:
\begin{align*}
  \nfun{ax}{XNorm} \colon \mathbb{R}^{\name{ax}} &\rightarrow \mathbb{R}^{\name{ax}} \\
  \nfun{ax}{XNorm}(X; \gamma, \beta, \epsilon) &= \frac{X - \nfun{ax}{mean}(X)}{\sqrt{\nfun{ax}{var}(X)} + \epsilon} \odot \gamma + \beta
\end{align*}
where
\begin{align*}
  \gamma, \beta &\in \mathbb{R}^{\name{ax}} \\
  \epsilon &> 0.
\end{align*}

Now, suppose that the input has three axes:
\begin{align*}
X &\in \reals^{{\name{batch} \times \name{chans} \times \name{layer}}}
\end{align*}
Then the three kinds of normalization layers can be written as:
\begin{align*}
Y &= \nfun{batch}{XNorm}(X; \gamma, \beta) && \text{batch normalization} \\
Y &= \nfun{layer}{XNorm}(X; \gamma, \beta) && \text{instance normalization} \\
Y &= \nfun{layer,chans}{XNorm}(X; \gamma, \beta) && \text{layer normalization}
\end{align*}

\subsection{Transformer}
\label{sec:transformer}

We define a Transformer used autoregressively as a language model.
The input is a sequence of one-hot vectors, from which we compute word embeddings and positional encodings:
\begin{align*}
  I &\in \{0, 1\}^{\name{seq} \times \name{vocab}} & \nsum{vocab} I &= 1 \\
  W &= (E \ndot{vocab} I)\sqrt{|\name{layer}|} & E &\in \reals^{\name{vocab} \times \name{layer}} \\
  P &\in \reals^{\name{seq} \times \name{layer}} \\
  P_{\nidx{seq}{p}, \nidx{layer}{i}} &= \begin{cases}
    \sin((p-1) / 10000^{(i-1) / |\name{layer}|}) & \text{$i$ odd} \\ 
    \cos((p-1) / 10000^{(i-2) / |\name{layer}|}) & \text{$i$ even.}
  \end{cases}
\end{align*}

Then we use $L$ layers of self-attention and feed-forward neural networks:
\begin{align*} 
X^0 &= W+P \\
T^1 &= \text{LayerNorm}^1(\text{SelfAtt}^1(X^0)) + X^0\\
X^1 &= \text{LayerNorm}^{1'}(\text{FFN}^1(T^1)) + T^1\\
&\vdotswithin{=} \\
T^{L} &= \text{LayerNorm}^L(\text{SelfAtt}^L(X^{L-1})) + X^{L-1}\\
X^{L} &= \text{LayerNorm}^{L'}(\text{FFN}^L(T^L)) + T^L\\
O &= \nfun{vocab}{softmax}(E \ndot{layer} X^L)
\end{align*}
where $\text{LayerNorm}$, $\text{SelfAtt}$ and $\text{FFN}$ are defined below.

Layer normalization ($l = 1, 1', \ldots, L, L'$):
\begin{align*}
  \text{LayerNorm}^l \colon \mathbb{R}^{\name{layer}} &\rightarrow \mathbb{R}^{\name{layer}} \\
  \text{LayerNorm}^l(X) &= \nfun{layer}{XNorm}(X; \beta^l, \gamma^l).
\end{align*}

We defined attention in \S\ref{sec:attention}; the Transformer uses multi-head self-attention, in which queries, keys, and values are all computed from the same sequence.
\begin{align*}
  \text{SelfAtt}^l \colon \mathbb{R}^{\name{seq} \times \name{layer}} &\rightarrow \mathbb{R}^{\name{seq} \times \name{layer}} \\
  \text{SelfAtt}^l(X) &= Y
\end{align*}
where
\begin{align*}
  |\name{seq}| &= |\name{seq'}| \\
  |\name{key}| = |\name{val}| &= |\name{layer}|/|\name{heads}| \\
  Q &= W^{l,Q} \ndot{layer} X_{\nmov{seq}{seq'}} & W^{l,Q} &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{key}} \\
  K &= W^{l,K} \ndot{layer} X & W^{l,K} &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{key}} \\
  V &= W^{l,V} \ndot{layer} X & W^{l,V} &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{val}} \\
  M & \in \reals^{\name{seq} \times \name{seq'}} \\
  M_{\nidx{seq}{i}, \nidx{seq'}{j}} &= \begin{cases}
    0 & i \leq j\\
    -\infty & \text{otherwise}
  \end{cases} \\
  Y &= W^{l,O} \ndot{heads,val} \text{Attention}(Q, K, V, M)_{\nmov{seq'}{seq}} & W^{l,O} &\in \mathbb{R}^{\name{heads} \times \name{val} \times \name{layer}}
\end{align*}

Feedforward neural networks:
\begin{align*}
  \text{FFN}^l \colon \mathbb{R}^{\name{layer}} &\rightarrow \mathbb{R}^{\name{layer}} \\
  \text{FFN}^l(X) &= X^2
\end{align*}
where
\begin{align*}
  X^1 &= \text{relu}(W^{l,1} \ndot{layer} X + b^{l,1}) & W^{l,1} &\in \mathbb{R}^{\name{hidden} \times \name{layer}} & b^{l,1} &\in \mathbb{R}^{\name{hidden}} \\
  X^2 &= \text{relu}(W^{l,2} \ndot{hidden} X^1 + b^{l,2}) & W^{l,2} &\in \mathbb{R}^{\name{layer} \times \name{hidden}} & b^{l,2} &\in \mathbb{R}^{\name{hidden}}.
\end{align*}

\subsection{LeNet}

\begin{align*}
X^0 &\in \reals^{\name{batch} \times \nset{chans}{c_0} \times \name{height} \times \name{width}} \\
T^1 &= \text{relu}(\text{Conv}^1(X^0)) \\
X^1 &= \text{MaxPool}^1(T^1) \\
T^2 &= \text{relu}(\text{Conv}^2(X^1)) \\
X^2 &= \text{MaxPool}^2(T^2)_{\nmov{(height,width,chans)}{layer}} \\
X^3 &= \text{relu}(W^3 \ndot{layer} X^2 + b^3) & W^3 &\in \mathbb{R}^{\name{hidden} \times \name{layer}} & b^3 &\in \mathbb{R}^{\name{hidden}} \\
O &= \nfun{classes}{softmax} (W^4 \ndot{hidden} X^3 + b^4) & W^4 &\in \mathbb{R}^{\name{classes} \times \name{hidden}} & b^4 &\in \mathbb{R}^{\name{classes}}
\end{align*}
As an alternative to the flattening operation in the equation for $X^2$, we could have written
\begin{align*}
X^2 &= \text{MaxPool}^2(T^2) \\
X^3 &= \text{relu}(W^3 \ndot{height,width,chans} X^2 + b^3) & W^3 &\in \mathbb{R}^{\name{hidden} \times \name{height} \times \name{width} \times \name{chans}}.
\end{align*}

The convolution and pooling operations are defined as follows:
\begin{align*}
\text{Conv}^l(X) &= \text{Conv2d}(X; W^l, b^l)_{\nmov{chans'}{chans}}
\end{align*}
where
\begin{align*}
W^l & \in \reals^{\nset{chans'}{c_{l}} \times \nset{chans}{c_{l-1}} \times \nset{kh}{kh_l} \times \nset{kw}{kw_l}} \\
b^l &\in \reals^{\nset{chans'}{c_{l}}}
\end{align*}
and
\begin{align*}
\text{MaxPool}^l(X) &= \text{MaxPol2d}_{ph^l,ph^l}(X).
\end{align*}

\subsection{Other examples}

\subsubsection{Discrete random variables}

Named axes are very helpful for working with discrete random variables, because each random variable can be represented by an axis with the same name. For instance, if $\name{A}$ and $\name{B}$ are random variables, we can treat $p(\name{B} \mid \name{A})$ and $p(\name{A})$ as tensors:
\begin{align*}
p(\name{B} \mid \name{A}) &\in [0, 1]^{\name{A} \times \name{B}} & \nsum{B} p(\name{B}\mid \name{A}) &= 1 \\
p(\name{A}) &\in [0, 1]^{\name{A}} & \nsum{A} p(\name{A}) &= 1
\end{align*}
Then many common operations on probability distributions can be expressed in terms of tensor operations:
\begin{align*}
p(\name{A}, \name{B}) &= p(\name{B} \mid \name{A}) \odot p(\name{A}) && \text{chain rule}\\
p(\name{B}) &= \nsum{A} p(\name{A}, \name{B}) = p(\name{B} \mid \name{A}) \ndot{A} p(\name{A}) && \text{marginalization} \\
p(\name{A} \mid \name{B}) &= \frac{p(\name{A}, \name{B})}{p(\name{B})} = \frac{p(\name{B} \mid \name{A}) \odot p(\name{A})}{p(\name{B} \mid \name{A}) \ndot{A} p(\name{A})}. && \text{Bayes' rule}
\end{align*}

\subsubsection{Continuous bag of words}

A continuous bag-of-words model classifies by summing up the embeddings of a sequence of words $X$ and then projecting them to the space of classes. 

\begin{align*}
\text{CBOW} \colon \{0, 1\}^{\name{seq} \times \name{vocab}} &\rightarrow \reals^{\name{seq} \times \name{classes}} \\
\text{CBOW}(X; E, W) &= \nfun{class}{softmax} (W \ndot{hidden} E \ndot{vocab} X)
\end{align*}
where
\begin{align*}
\nsum{vocab} X &= 1 \\
E &\in \reals^{\name{vocab} \times \name{hidden}} \\
W &\in \reals^{\name{classes} \times \name{hidden}}.
\end{align*}
Here, the two contractions can be done in either order, so we leave the parentheses off.

\subsubsection{Sudoku ILP}

Sudoku puzzles can be represented as  binary tiled tensors.
Given a grid we can check that it is valid by converting it to a grid of grids. 
Constraints then ensure that there is one digit per row, per column and per sub-box.

\begin{align*}
\text{check} \colon \{0, 1\}^{\nset{height}{9} \times \nset{width}{9} \times \nset{assign}{9}} &\rightarrow \{0, 1\} \\
\text{check}(X) &=
\mathbb{I}\left[\begin{aligned}
\nsum{assign} X = 1 &\land \nsum{height, width} Y = 1 \land {} \\
\nsum{height} X = 1 &\land \nsum{width} X = 1
\end{aligned}\right]
\end{align*}
where
\begin{align*}
Y &\in \{0, 1\}^{\nset{height'}{3} \times \nset{width'}{3} \times \nset{height}{3} \times \nset{width}{3} \times \nset{assign}{9}}  \\
Y_{\nidx{height'}{h'}, \nidx{height}{h}, \nidx{width'}{w'}, \nidx{width}{w}} &= X_{\nidx{height}{3h' + h-1}, \nidx{width}{3 w' + w-1}}.
\end{align*} 

\subsubsection{$K$-means clustering}

The following equations define one step of $k$-means clustering. Given a set of points $X$ and an initial set of cluster centers $C$,
\begin{align*}
  X &\in \reals^{\name{batch} \times \name{space}} \\
C &\in \reals^{\name{clusters} \times \name{space}}
\end{align*}
we repeat the following update: Compute cluster assignments
\begin{align*}
Q &= \nfun{clusters}{argmin} \nfun{space}{norm}(C-X)
\end{align*}
then recompute the cluster centers:
\begin{equation*}
C \leftarrow \nsum{batch} \frac{Q \odot X}{Q}.
\end{equation*}

\subsubsection{Beam search}

Beam search is a commonly used approach for approximate discrete search. Here $H$ is the score of each element in the beam, $S$ is the state of each element in the beam, and $f$ is an update function that returns the score of each state transition. 
\begin{align*} 
H &\in \reals^{\name{beam}} \\
S &\in \{0, 1\}^{\name{beam} \times \name{state}} & \nsum{state} S &= 1 \\
f &\colon \{0, 1\}^{\name{state}} \rightarrow \reals^{\name{state}} \\
\end{align*}
Then we repeat the following update:
\begin{align*}
H' &= \nfun{beam}{max} (H \odot f(S)) \\
H &\leftarrow \nfun{state,beam}{maxk} H' \\
S &\leftarrow \nfun{state,beam}{argmaxk} H'
\end{align*}
where
\begin{align*}
\nfun{ax,k}{maxk} \colon \reals^{\name{ax}} &\rightarrow \reals^{\name{k}} \\
\nfun{ax,k}{argmaxk} \colon \reals^{\name{ax}} &\rightarrow \{0,1\}^{\name{ax},\name{k}}
\end{align*}
are defined such that $[\nfun{ax,k}{maxk} A]_{\nidx{k}{i}}$ is the $i$-th largest value along axis $\name{ax}$ and $A \ndot{ax} (\nfun{ax,k}{argmaxk}{A}) = \nfun{ax,k}{max} A$.

We can add a \name{batch} axis to $H$ and $S$ and the above equations will work unchanged.

\subsubsection{Multivariate normal distribution}

To define a multivariate normal distribution, we need some matrix operations. These have two axis names written under them, for rows and columns, respectively. Determinant and inverse have the following signatures:
\begin{align*}
\nfun{ax1,ax2}{det} \colon F^{\nset{ax1}{n} \times \nset{ax2}{n}} &\rightarrow F \\
\nfun{ax1,ax2}{inv} \colon F^{\nset{ax1}{n} \times \nset{ax2}{n}} &\rightarrow F^{\nset{ax1}{n} \times \nset{ax2}{n}}.
\end{align*}
(We write $\text{inv}$ instead of $\cdot^{-1}$ because there's no way to write axis names under the latter.)

In our notation, the application of a bilinear form is more verbose than the standard notation ($(X-\mu)^\top \Sigma^{-1} (X-\mu)$), but also makes it look more like a function of two arguments (and would generalize to three or more arguments).

\begin{align*}
\mathcal{N} \colon \reals^{\name{d}} &\rightarrow \reals \\
\mathcal{N}(X; \mu, \Sigma) &= \frac{\exp\left(-\frac{1}{2} \left(\nfun{d1, d2}{inv} \Sigma\right) \ndot{d1,d2} \left([X - \mu]_{\nmov{d}{d1}} \odot [X - \mu]_{\nmov{d}{d2}} \right) \right)}{\sqrt{(2 \pi)^{|\name{d}|} \nfun{d1, d2}{det} \Sigma}}
\end{align*}
where
\begin{align*}
|\name{d}| &= |\name{d1}| = |\name{d2}| \\
\mu &\in \reals^{\name{d}} \\
\Sigma & \in \reals^{\name{d1} \times \name{d2}}.
\end{align*}

\section{\LaTeX{} Macros}

Many of the \LaTeX{} macros used in this document are available in the style file \url{https://namedtensor.github.io/namedtensor.sty}. To use it, put
\begin{quote}
\begin{verbatim}
\usepackage{namedtensor}
\end{verbatim}
\end{quote}
in the preamble of your \LaTeX{} source file (after \verb|\documentclass{article}| but before \verb|\begin{document}|).

The style file contains a small number of macros:
\begin{itemize}
\item Basics
  \begin{itemize}
  \item Use \verb|\name{foo}| to write an axis name: $\name{foo}$.
  \item Use \verb|\mathbb{R}^{\nset{foo}{2}}| to write a set of tensors: $\mathbb{R}^{\nset{foo}{2}}$.
  \item Use \verb|A_{\nidx{foo}{1}}| to index a tensor: $A_{\nidx{foo}{1}}$.
  \item Use \verb|A_{\nmov{foo}{bar}}| for renaming: $A_{\nmov{foo}{bar}}$.
  \end{itemize}
\item Binary operators
  \begin{itemize}
  \item Use \verb|A \ndot{foo} B| for contraction: $A \ndot{foo} B$.
  \item Use \verb|A \ncat{foo} B| for concatenation: $A \ncat{foo} B$.
  \item In general, you can use \verb|\nbin| to make a new binary operator with a name under it: \verb|A \nbin{foo}{\star} B| gives you $A \nbin{foo}{\star} B$.
  \end{itemize}
\item Functions
  \begin{itemize}
  \item Use \verb|\nsum{foo} A| for summation: $\nsum{foo} A$.
  \item In general, you can use \verb|\nfun| to make a function with a name under it: \verb|\nfun{foo}{qux} A| gives you $\nfun{foo}{qux} A$.
  \end{itemize}
\end{itemize}

\section{Formal Definitions}
\label{sec:definitions}

\subsection{Records and shapes}

A \emph{named index} is a pair, written $\nidx{ax}{i}$, where $\name{ax}$ is a \emph{name} and $i$ is usually a natural number. We write both names and variables ranging over names using sans-serif font.

A \emph{record} is a set of named indices $\{\nidx{ax\sub{1}}{i_1}, \ldots, \nidx{ax\sub{r}}{i_r}\}$, where $\name{ax\sub{1}}, \ldots \name{ax\sub{r}}$ are pairwise distinct names. 

An \emph{axis} is a pair, written $\nset{ax}{I}$, where $\name{ax}$ is a name and $I$ is a set of \emph{indices}.
We deal with axes of the form $\nset{ax}{[n]}$ (that is, $\nset{ax}{\{1, \ldots, n\}}$) so frequently that we abbreviate this as~$\nset{ax}{n}$.

In many contexts, there is only one axis with name $\name{ax}$, and so we refer to the axis simply as $\name{ax}$. The context always makes it clear whether $\name{ax}$ is a name or an axis. If $\name{ax}$ is an axis, we write $\ind(\name{ax})$ for its index set, and we write $|\name{ax}|$ as shorthand for~$|\ind(\name{ax})|$.

A \emph{shape} is a set of axes, written $\nset{ax\sub{1}}{I_1} \times \cdots \times \nset{ax\sub{r}}{I_r}$, where $\name{ax\sub{1}}, \ldots \name{ax\sub{r}}$ are pairwise distinct names. We write $\emptyset$ for the empty shape. A shape defines a set of records:
\begin{equation*}
\rec (\nset{ax\sub{1}}{I_1} \times \cdots \times \nset{ax\sub{r}}{I_r}) = \left\{\{\nidx{ax\sub{1}}{i_1}, \ldots, \nidx{ax\sub{r}}{i_r}\} \mid i_1 \in I_1, \ldots, i_r \in I_r\right\}.
\end{equation*}

We say two shapes $\mathcal{S}$ and $\mathcal{T}$ are \emph{compatible} if whenever $\nset{ax}{I} \in \mathcal{S}$ and $\nset{ax}{J} \in \mathcal{T}$, then $I = J$. We say that $\mathcal{S}$ and $\mathcal{T}$ are \emph{orthogonal} if there is no $\name{ax}$ such that $\nset{ax}{I} \in \mathcal{S}$ and $\nset{ax}{J} \in \mathcal{T}$ for any $I$, $J$.

If $t \in \rec \mathcal{T}$ and $\mathcal{S} \subseteq \mathcal{T}$, then we write $\restrict{t}{\mathcal{S}}$ for the unique record in $\rec \mathcal{S}$ such that $\restrict{t}{\mathcal{S}} \subseteq t$.

\subsection{Named tensors}

Let $F$ be a field and let $\mathcal{S}$ be a shape. Then a \emph{named tensor over $F$ with shape $\mathcal{S}$} is a mapping from $\mathcal{S}$ to $F$. We write the set of all named tensors with shape $\mathcal{S}$ as $F^{\mathcal{S}}$.

We don't make any distinction between a scalar (an element of $F$) and a named tensor with empty shape (an element of $F^\emptyset$).

If $A \in F^{\mathcal{S}}$, then we access an element of $A$ by applying it to a record $s \in \rec \mathcal{S}$; but we write this using the usual subscript notation: $A_s$ rather than $A(s)$. To avoid clutter, in place of $A_{\{\nidx{ax\sub{1}}{x_1}, \ldots, \nidx{ax\sub{r}}{x_r}\}}$, we usually write $A_{\nidx{ax\sub{1}}{x_1}, \ldots, \nidx{ax\sub{r}}{x_r}}$. When a named tensor is an expression like $(A+B)$, we surround it with square brackets like this: $[A+B]_{\nidx{ax\sub{1}}{x_1}, \ldots, \nidx{ax\sub{r}}{x_r}}$.

We also allow partial indexing. If $A$ is a tensor with shape $\mathcal{T}$ and $s \in \rec \mathcal{S}$ where $\mathcal{S} \subseteq \mathcal{T}$, then we define $A_s$ to be the named tensor with shape $\mathcal{T} \setminus \mathcal{S}$ such that, for any $t \in \rec (\mathcal{T} \setminus \mathcal{S})$,
\begin{align*}
\left[A_s\right]_t &= A_{s \cup t}.
\end{align*}
(For the edge case $\mathcal{T} = \emptyset$, our definitions for indexing and partial indexing coincide: one gives a scalar and the other gives a tensor with empty shape, but we don't distinguish between the two.)

\subsection{Named tensor operations}
\label{sec:tensorfunctions}

In \S\ref{sec:overview}, we described several classes of functions that can be extended to named tensors. Here, we define how to do this for general functions.

Let $f \colon F^{\mathcal{S}} \rightarrow G^{\mathcal{T}}$ be a function from tensors to tensors. For any shape $\mathcal{S'}$ orthogonal to both $\mathcal{S}$ and $\mathcal{T}$, we can extend $f$ to:
\begin{align*}
f \colon F^{\mathcal{S} \cup \mathcal{S'}} &\rightarrow G^{\mathcal{T} \cup \mathcal{S'}} \\
[f(A)]_r &= f(A_r) \qquad \text{for all $r \in \rec\mathcal{S'}$.}
\end{align*}

If $f$ is a multary function, we can extend its arguments to larger shapes, and we don't have to extend all the arguments with the same names. We consider just the case of two arguments; three or more arguments are analogous. Let $f \colon F^{\mathcal{S}} \times G^{\mathcal{T}} \rightarrow H^{\mathcal{U}}$ be a binary function from tensors to tensors. For any shapes $\mathcal{S'}$ and $\mathcal{T'}$ that are compatible with each other and orthogonal to $\mathcal{S}$ and $\mathcal{T}$, respectively, and $\mathcal{S'} \cup \mathcal{T'}$ is orthogonal to $\mathcal{U}$, we can extend $f$ to:
\begin{align*}
f \colon F^{\mathcal{S} \cup \mathcal{S'}} \times G^{\mathcal{T} \cup \mathcal{T'}} &\rightarrow H^{\mathcal{U} \cup \mathcal{S'} \cup \mathcal{T'}} \\
  [f(A,B)]_r &= f\left(A_{\restrict{r}{\mathcal{S'}}},B_{\restrict{r}{\mathcal{T'}}}\right) \qquad \text{for all $r \in \rec (\mathcal{S'} \cup \mathcal{T'})$.}
\end{align*}

All of the tensor operations described in \S\ref{sec:operations} can be defined in this way. For example, the contraction operator can be defined as:
\begin{align*}
  \ndot{ax} \colon F^{\nset{ax}{n}} \times F^{\nset{ax}{n}} &\rightarrow F \\
  A \ndot{ax} B &= \sum_{i=1}^n A_{\nidx{ax}{i}} B_{\nidx{ax}{i}}.
\end{align*}

\section{Extensions}

\input{types}
\input{fancy}
\input{calculus}

\section{Alternatives}

\input{alternatives}

\section*{Acknowledgements}

Thanks to Ekin Aky\"{u}rek, Colin McDonald, Adam Poliak, Matt Post, Chung-chieh Shan, Nishant Sinha, and Yee Whye Teh for their input to this document (or the ideas in it).

\iffalse % hack to make this heading appear only in pandoc
\section*{References}
\fi

\bibliographystyle{acl_natbib}
\bibliography{references}

\end{document}
