\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}

\usepackage{namedtensor}
% "experimental" notation
\newcommand{\nndot}[2]{\mathbin{\mathop{\boldsymbol\cdot}\limits_{\name{#1}|\name{#2}}}}

% for formal definitions
\newcommand{\tuple}[1]{\{#1\}}
\DeclareMathOperator{\tupleshape}{ind}
\newcommand{\tuplerestrict}[2]{\left.#1\right|_{#2}}
\newcommand{\nmatrix}[3]{\name{#1}\begin{array}[b]{@{}c@{}}\name{#2}\\\begin{bmatrix}#3\end{bmatrix}\end{array}}

\DeclareMathOperator*{\softmax}{softmax}
\newcommand{\reals}{\mathbb{R}}

\usepackage{parskip}
\setcounter{tocdepth}{2}
\usepackage{natbib}
\usepackage[hidelinks]{hyperref}

\title{Named Tensor Notation}
\author{David Chiang and Sasha Rush}

\begin{document}

\maketitle

\tableofcontents

\clearpage

\section{Introduction}

Most papers about neural networks use the notation of vectors and matrices from applied linear algebra. This notation is very well-suited to talking about vector spaces, but less well-suited to talking about neural networks. Consider the following equation \citep{vaswani+:2017}:
\[ \text{Att}(Q, K, V) = \softmax \left( \frac{QK^\top}{\sqrt{|\name{key}|}} \right) V. \]
where $Q$, $K$, and $V$ are sequences of query, key, and value vectors packed into matrices. Does the product $QK^\top$ sum over the sequence, or over the query/key features? We would need to know the sizes of $Q$, $K$, and $V$ to know that it's taken over the query/key features. Is the softmax taken over the query sequence or the key sequence? The usual notation doesn't even offer a way to answer this question. With multiple attention heads, the notation becomes more complicated and leaves more questions unanswered. With multiple sentences in a minibatch, the notation becomes more complicated still, and most papers wisely leave this detail out.

Libraries for programming with neural networks \citep{numpy,pytorch} provide multidimensional arrays, called tensors (although usually without the theory associated with tensors in linear algebra and physics), and a rich array of operations on tensors. But they inherit from math the convention of identifying indices by \emph{position}, making code bug-prone. Quite a few libraries have been developed to identify indices by \emph{name} instead: Nexus \citep{chen2017typesafe}, tsalib \citep{tsalib}, NamedTensor \citep{namedtensor}, named tensors in PyTorch \citep{named-tensors}, and Dex \citep{maclaurin+:2019}. (Some of these libraries also add types to indices, but here we are only interested in adding names.)

Back in the realm of mathematical notation, then, we want two things: first, the flexibility of working with multidimensional arrays, and second, the perspicuity of identifying indices by name instead of by position. This document describes our proposal to do both.

As a preview, the above equation becomes
\begin{equation*}
  \text{Att}(Q,K,V) = \nfun{seq}{softmax} \left( \frac{Q \ndot{key} K}{\sqrt{|\name{key}|}} \right) \ndot{seq} V
\end{equation*}
making it unambiguous which index each operation applies to. The same equation works with multiple heads and with minibatching.

More examples of the notation are given in \S\ref{sec:examples}.

The source code for this document can be found at \url{https://github.com/namedtensor/notation/}. We invite anyone to make comments on this proposal by submitting issues or pull requests on this repository.

\section{Informal Overview}
\label{sec:intro}

Let's think first about the usual notions of vectors, matrices, and tensors, without named indices.

Define $[n] = \{1, \ldots, n\}$. We can think of a size-$n$ real vector $v$ as a function from $[n]$ to $\mathbb{R}$. We get the $i$th element of $v$ by applying $v$ to $i$, but we normally write this as $v_i$ (instead of $v(i)$). 

Similarly, we can think of an $m \times n$ real matrix as a function from $[m] \times [n]$ to $\mathbb{R}$, and an $l \times m \times n$ real tensor as a function from $[l] \times [m] \times [n]$ to $\mathbb{R}$. In general, then, real tensors are functions from \emph{tuples of natural numbers} to reals.

\subsection{Named tensors}

We want to make tensors into functions, no longer on tuples, but on \emph{records}, which look like this: \[\tuple{\nidx{foo}{1}, \nidx{bar}{3}}\] where $\name{foo}$ and $\name{bar}$ are \emph{names} (written in sans-serif font), mapped to 1 and 3, respectively. The pairs $\nidx{foo}{1}$ and $\nidx{bar}{3}$ are called \emph{named indices}. Their order doesn't matter: $\tuple{\nidx{foo}{1}, \nidx{bar}{3}}$ and $\tuple{\nidx{bar}{3}, \nidx{foo}{1}}$ are the same record.

The set of records that can be used to index a named tensor is defined by a \emph{named shape}, which looks like this: \[\nset{foo}{2} \times \nset{bar}{3}\] which stands for records where index $\name{foo}$ ranges from 1 to 2, and index $\name{bar}$ ranges from 1 to 3. Again, the order of the elements, called \emph{named index sets}, doesn't matter: $\nset{foo}{2} \times \nset{bar}{3}$ and $\nset{bar}{3} \times \nset{foo}{2}$ are the same shape.

Then, a real \emph{named tensor} is a function from (the set of records defined by) a named shape to the real numbers. For example, here is a tensor with shape $\nset{foo}{2} \times \nset{bar}{3}$.
\begin{equation*}
A = \nmatrix{foo}{bar}{
  3 & 1 & 4 \\
  1 & 5 & 9
}.
\end{equation*}

We access elements of $A$ using subscripts: $A_{\nidx{foo}{1}, \nidx{bar}{3}} = 4$.
We also allow partial indexing:
\begin{align*}
A_{\nidx{foo}{1}} &= \nmatrix{}{bar}{
  3 & 1 & 4
}
&
A_{\nidx{bar}{3}} &= \nmatrix{}{foo}{
  4 & 9
}.
\end{align*}

We use uppercase italic letters for variables standing for named tensors. We don't mind if you use another convention, but urge you not to use different styles for tensors and their elements. For example, if $\mathbf{A}$ is a tensor, then an element of $\mathbf{A}$ is written as $\mathbf{A}_{\nidx{foo}{2}, \nidx{bar}{3}}$ -- 
not $A_{\nidx{foo}{2},\nidx{bar}{3}}$ or $a_{\nidx{foo}{2},\nidx{bar}{3}}$.

Just as the set of all size-$n$ real vectors is written $\mathbb{R}^n$, and the set of all $m\times n$ real matrices is often written $\mathbb{R}^{m \times n}$ (which makes sense because one sometimes writes $Y^X$ for the set of all functions from $X$ to $Y$), we write $\mathbb{R}^{\nset{foo}{2} \times \nset{bar}{3}}$ for the set of all tensors with shape $\nset{foo}{2} \times \nset{bar}{3}$.

It's very common for an index name to be used with only one size. If you write
\begin{align*}
\name{foo} &\ndef [2] \\
\name{bar} &\ndef [3]
\end{align*}
then you can simply write $\mathbb{R}^{\name{foo} \times \name{bar}}$ for the set of tensors with index $\name{foo}$ ranging from 1 to 2 and $\name{bar}$ ranging from 1 to 3. We write $|\name{foo}|$ for the size of $\name{foo}$ (in this example, $|\name{foo}| = 2$). For more information on what $\ndef$ does, please see \S\ref{sec:definitions}.

What are good choices for index names? We recommend meaningful \emph{words} instead of single letters, and we recommend words that describe a \emph{whole} rather than its parts. For example, a minibatch of sentences, each of which is a sequence of one-hot vectors, would be represented by a tensor with three indices, which we might name $\name{batch}$, $\name{seq}$, and $\name{vocab}$. Please see \S\ref{sec:examples} for more examples.

\subsection{Named tensor operations}
\label{sec:operations}

\subsubsection{Elementwise operations}

Any function from scalars to scalars can be applied elementwise to a named tensor:
\begin{equation*}
\exp A = \nmatrix{foo}{bar}{
  \exp 3 & \exp 1 & \exp 4 \\
  \exp 1 & \exp 5 & \exp 9
}.
\end{equation*}
More elementwise unary operations:
\[\begin{array}{cl}
kA & \text{scalar multiplication by $k$} \\
-A & \text{negation} \\
A^k & \text{elementwise exponentiation} \\
\sqrt{A} &\text{elementwise square root} \\
\exp A & \text{elementwise exponential function} \\
\tanh A & \text{hyperbolic tangent} \\
\sigma(A) & \text{logistic sigmoid} \\
\text{ReLU}(A) & \text{rectified linear unit}
\end{array}\]

Any function or operator that takes two scalar arguments can be applied elementwise to two named tensors with the same shape. If $A$ is as above and
\begin{equation*}
B = \nmatrix{foo}{bar}{
  2 & 7 & 1 \\
  8 & 2 & 8
}
\end{equation*}
then
\begin{equation*}
A + B = \nmatrix{foo}{bar}{
  3+2 & 1+7 & 4+1 \\
  1+8 & 5+2 & 9+8
}.
\end{equation*}

But things get more complicated when $A$ and $B$ don't have the same shape. If $A$ and $B$ each have an index with the same name (and size), the two indices are \emph{aligned}, as above. But if $A$ has an index named $\name{i}$ and $B$ doesn't, then we do \emph{broadcasting}, which means effectively that we replace $B$ with a new tensor $B'$ that contains a copy of $B$ for every value of index $\name{i}$.
\begin{align*}
A + 1 &= \nmatrix{foo}{bar}{
  3+1 & 1+1 & 4+1 \\
  1+1 & 5+1 & 9+1
} \\
A + B_{\nidx{foo}{1}} &= \nmatrix{foo}{bar}{
  3+2 & 1+7 & 4+1 \\
  1+2 & 5+7 & 9+1
} \\
A + B_{\nidx{bar}{3}} &= \nmatrix{foo}{bar}{
  3+1 & 1+1 & 4+1 \\
  1+8 & 5+8 & 9+8
}.
\end{align*}
Similarly, if $B$ has an index named $\name{i}$ and $A$ doesn't, then we effectively replace $A$ with a new tensor $A'$ that contains a copy of $A$ for every value of index $\name{i}$. If you've programmed with NumPy or any of its derivatives, this should be unsurprising to you.

More elementwise binary operations:
\[\begin{array}{cl}
A+B & \text{addition} \\
A-B & \text{subtraction} \\
A\odot B & \text{elementwise (Hadamard) product} \\
\displaystyle\frac{A}{B} & \text{elementwise division} \\[1.2ex]
\max \{A, B\} & \text{elementwise maximum} \\
\min \{A, B\} & \text{elementwise minimum}
\end{array}\]

\subsubsection{Reductions}

The same rules for alignment and broadcasting apply to functions that take tensor as arguments or return tensors. The gory details are in \S\ref{sec:tensorfunctions}, but we present the most important subcases here. The first is \emph{reductions}, which are functions from vectors to scalars. Unlike with functions on scalars, we always have to specify which index these functions apply to, using a subscript. (This is equivalent to the \verb|axis| argument in NumPy and \verb|dim| in PyTorch.)

For example, using the same example tensor $A$ from above,
\begin{align*}
\nsum{foo} A &= \nmatrix{}{bar}{
  3+1 & 1+5 & 4+9
} \\
\nsum{bar} A &= \nmatrix{}{foo}{
  3+1+4 & 1+5+9
}.
\end{align*}
More reductions: If $A$ has shape $\nset{foo}{I} \times \ldots$, then
\begin{align*}
  \nsum{foo} A &= \sum_{i \in I} A_{\nidx{foo}{i}} = \nmatrix{}{bar}{4 & 6 & 13} \\
  \nfun{foo}{norm} A &= \sqrt{\nsum{foo} A^2} = \nmatrix{}{bar}{\sqrt{10} & \sqrt{26} & \sqrt{97}} \\
  \nfun{foo}{min} A &= \min_{i \in I} A_{\nidx{foo}{i}} = \nmatrix{}{bar}{1 & 1 & 4} \\
  \nfun{foo}{max} A &= \max_{i \in I} A_{\nidx{foo}{i}} = \nmatrix{}{bar}{3 & 5 & 9} \\
  \nfun{foo}{mean} A &= \frac{1}{|I|} A = \nmatrix{}{bar}{2 & 3 & 6.5} \\
  \nfun{foo}{var} A &= \frac1{|I|} \nsum{i} (A - \nfun{foo}{mean} A)^2 = \nmatrix{}{bar}{1 & 4 & 6.25}
\end{align*}
(Note that $\max$ and $\min$ are overloaded; with multiple arguments and no subscript, they are elementwise, and with a single argument and a subscript, they are reductions.)

You can also write multiple names to perform the reduction over multiple indices at once.

\subsubsection{Contraction}

The vector dot product (inner product) is a function from \emph{two} vectors to a scalar, which generalizes to named tensors to give the ubiquitous \emph{contraction} operator, which performs elementwise multiplication, then sums along an index. It can be used, for example, for matrix multiplication:
\begin{align*}
C &= \nmatrix{bar}{baz}{
  1 & -1 \\ 2 & -2 \\ 3 & -3
} \\
A \ndot{bar} C &= \nmatrix{foo}{baz}{
  17 & -17 \\
  53 & -53
}
\end{align*}
However, note that (like vector dot-product, but unlike matrix multiplication) this operator is commutative, but not associative! Specifically, if
\begin{align*}
A &\in \mathbb{R}^{\nset{foo}{m}} \\
B &\in \mathbb{R}^{\nset{foo}{m} \times\nset{bar}{n}} \\
C &\in \mathbb{R}^{\nset{foo}{m} \times\nset{bar}{n}}
\end{align*}
then $(A \ndot{foo} B) \ndot{bar} C$ and $A \ndot{foo} (B \ndot{bar} C)$ don't even have the same shape.

\subsubsection{Vectors to vectors}

A very common example of a function from vectors to vectors is the softmax:
\begin{equation*}
  \nfun{foo}{softmax} A = \frac{\exp A}{\nsum{foo} \exp A} \approx \nmatrix{foo}{bar}{
    0.731 & 0.002 & 0.953 \\
    0.269 & 0.998 & 0.047
  }.
\end{equation*}

And it's also very handy to have a function that renames an index:
\begin{equation*}
\nmov{bar}{baz}{A} = \nmatrix{foo}{baz}{
  3 & 1 & 4 \\
  1 & 5 & 9
}
\end{equation*}

Concatenation combines two vectors into one:
\begin{align*}
  A \ncat{foo} B &= \nmatrix{foo}{bar}{
    3 & 1 & 4 \\
    1 & 5 & 9 \\
    2 & 7 & 1 \\
    8 & 2 & 8
  } \\
  A \ncat{bar} B &= \nmatrix{foo}{bar}{
    3 & 1 & 4 & 2 & 7 & 1 \\
    1 & 5 & 9 & 8 & 2 & 8
  }
\end{align*}

\subsubsection{Matrices}

Finally, we briefly consider functions on matrices, for which you have to give \emph{two} index names (and the order in general matters). Let $A$ be a named tensor with shape $\{\nset{foo}{2} \times\nset{bar}{2} \times\nset{baz}{2\}}$:
\begin{align*}
A_{\nidx{foo}{1}} &= \nmatrix{bar}{baz}{
  1 & 2 \\
  3 & 4
} \\
A_{\nidx{foo}{2}} &= \nmatrix{bar}{baz}{
  5 & 6 \\
  7 & 8
} \\
\nfun{bar,baz}{det} A &= \nmatrix{}{foo}{\det \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} & \det \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix}} \\
\nfun{baz,bar}{det} A &= \nmatrix{}{foo}{\det \begin{bmatrix} 1 & 3 \\ 2 & 4 \end{bmatrix} & \det \begin{bmatrix} 5 & 7 \\ 6 & 8 \end{bmatrix}} \\
\nfun{foo,bar}{det} A &= \nmatrix{}{baz}{\det \begin{bmatrix} 1 & 3 \\ 5 & 7 \end{bmatrix} & \det \begin{bmatrix} 2 & 4 \\ 6 & 8 \end{bmatrix}}
\end{align*}
For matrix inverses, there's no easy way to put a subscript under $\mathord\cdot^{-1}$, so we recommend writing $\nfun{foo,bar}{inv}$.

\section{Examples}
\label{sec:examples}

In this section we give a series of examples illustrating how to use named tensors in various situations, mostly related to machine learning.

\subsection{Building blocks}

\subsubsection{Fully-connected layers}

We use rectified linear units (ReLUs) because some of the larger models we define later require it; a logistic function or tanh would also work here.
\begin{align*}
  \text{Full} &\colon \reals^{\name{layer}} \rightarrow \reals^{\name{layer'}} \\
  \text{Full}(x; W, b) &= \max \left\{ 0, W \ndot{layer} x + b \right\}
\end{align*}
where
\begin{align*}
  W &\in \reals^{\name{layer'} \times \name{layer}} \\
  b &\in \reals^{\name{layer'}} \\
\end{align*}
  
\subsubsection{Recurrent neural networks}
\label{sec:rnn}

As a second example, let's define a simple (Elman) RNN.
\begin{align*}
x^{(t)} &\in \mathbb{R}^{\name{emb}} & t &= 1, \ldots, n \\
h^{(t)} &\in \mathbb{R}^{\name{state}} & t &= 0, \ldots, n\\
A &\in \mathbb{R}^{\name{state} \times \name{state'}} & |\name{state}| &= |\name{state'}| \\
B &\in \mathbb{R}^{\name{emb} \times \name{state'}} \\
c &\in \mathbb{R}^{\name{state'}} \\
h^{(t+1)} &= \nmov{state'}{state}{\tanh\left( A \ndot{state} h^{(t)} + B \ndot{emb} x^{(t)} + c \right)}
\end{align*}
Here the index name $\name{state}$ has to stay the same across time, and the renaming is necessary because our notation doesn't provide a one-step way to apply a linear transformation ($A$) to one index and put the result in the same index. For possible solutions, see \S\ref{sec:duality}.

\subsubsection{Attention}
\label{sec:attention}

Define a function
\begin{align*}
  \text{Att} &\colon \mathbb{R}^{\name{key}} \times \mathbb{R}^{\name{seq} \times\name{key}} \times \mathbb{R}^{\name{seq} \times\name{val}} \rightarrow \mathbb{R}^{\name{val}} \\
  \text{Att}(Q,K,V) &= \nfun{seq}{softmax} \left( \frac{Q \ndot{key} K}{\sqrt{|\name{key}|}} \right) \ndot{seq} V
\end{align*}

Sometimes we need to apply a mask to keep from attending to certain positions.
\begin{align*} 
  \text{Att} &\colon \mathbb{R}^{\name{key}} \times \mathbb{R}^{\name{seq} \times\name{key}} \times \mathbb{R}^{\name{seq} \times\name{val}} \times \mathbb{R}^{\name{seq}} \rightarrow \mathbb{R}^{\name{val}} \\
\text{Att}(Q, K, V, M) &=  \nfun{seq}{softmax} \left( \frac{Q \ndot{key} K}{\sqrt{|\name{key}|}} + M \right) \ndot{seq} V 
\end{align*}

If $Q$, $K$, and $V$ have a \name{head} index for multiple attention heads, then the above equations will compute multi-head attention without modification.

\subsubsection{Convolution}

A 1-dimensional convolution can be easily written by unrolling a tensor and then
applying a standard dot product.
\begin{align*}
X &\in \reals^{\name{channels} \times \nset{seq}{n}}  \\
W &\in \reals^{\name{channels} \times \name{kernel}} &
b &\in \reals \\
\text{conv1d}(X; W, b) &= W \ndot{channels, kernel} U + b \\
U &\in \reals^{\name{channels} \times \nset{seq}{n-|\name{kernel}|+1} \times \name{kernel}}  \\
U_{\nidx{seq}{i}, \nidx{kernel}{j}} & = X_{\nidx{seq}{i+j - 1}}  
\end{align*} 

A 2-dimensional convolution:
\begin{align*}
X &\in \reals^{\name{channels} \times \nset{height}{h} \times \nset{width}{w}} \\
W &\in \reals^{\name{channels} \times \name{kh} \times \name{kw}} &
b &\in \reals \\
\text{conv2d}(X; W, b) &=  W \ndot{channels, kh, kw} U + b \\
U &\in \reals^{\name{channels} \times \nset{height}{h-|\name{kh}|+1} \times
\nset{width}{w-|\name{kw}|+1} \times \name{kh} \times \name{kw}}  \\
U_{\nidx{height}{i}, \nidx{width}{j}, \nidx{kh}{ki}, \nidx{kw}{kj}} &= X_{\nidx{height}{i+ki-1}, \nidx{width}{j+kj-1}}  
\end{align*}

\subsubsection{Max pooling}

\begin{align*}
X &\in \reals^{\nset{height}{h} \times \nset{width}{w}} \\
\text{maxpool2d}(X, kh, kw) &=  \nfun{kh, kw}{max} U \\
U &\in \reals^{{\nset{height}{h / kh} \times \nset{width}{w / kw} \times \nset{kh}{kh} \times \nset{kw}{kw}}} \\
U_{\nidx{height}{i}, \nidx{width}{j}, \nidx{kh}{di}, \nidx{kw}{dj}} & = X_{\nidx{height}{i \times kh + di -1}, \nidx{width}{j \times kw + dj -1}}  
\end{align*}

\subsubsection{Normalization layers}

Batch, instance, and layer normalization are often informally described using the same
equation, but they each correspond to very different functions. They differ
by which axes are normalized.

We can define a single generic normalization layer:
\begin{align*}
  \nfun{d}{xnorm} &\colon \mathbb{R}^{\name{d}} \rightarrow \mathbb{R}^{\name{d}} \\
  \nfun{d}{xnorm}(X; \gamma, \beta, \epsilon) &= \frac{X - \nfun{d}{mean}(X)}{\sqrt{\nfun{d}{var}(X)} + \epsilon} \odot \gamma + \beta
\end{align*}
where
\begin{align*}
  \gamma, \beta &\in \mathbb{R}^{\name{d}} \\
  \epsilon &> 0
\end{align*}

Now, suppose that the input has three indices:
\begin{align*}
X &\in \reals^{{\name{batch} \times \name{channels} \times \name{layer}}}
\end{align*}
Then the three kinds of normalization layers can be written as:
\begin{align*}
Y &= \nfun{batch}{xnorm}(X; \gamma, \beta) && \text{batch normalization} \\
Y &= \nfun{layer}{xnorm}(X; \gamma, \beta) && \text{instance normalization} \\
Y &= \nfun{layer,channels}{xnorm}(X; \gamma, \beta) && \text{layer normalization}
\end{align*}

\subsection{Discrete random variables}

Named indices are very helpful for working with discrete random variables, because each random variable can be represented by an index with the same name. For instance, if $\name{A}$ and $\name{B}$ are random variables, we can treat $p(\name{B} \mid \name{A})$ and $p(\name{A})$ as tensors:
\begin{align*}
p(\name{B} \mid \name{A}) &\in [0, 1]^{\name{A} \times \name{B}} & \nsum{B} p(\name{B}\mid \name{A}) &= 1 \\
p(\name{A}) &\in [0, 1]^{\name{A}} & \nsum{A} p(\name{A}) &= 1
\end{align*}
Then Bayes' rule is just:
\begin{align*}
p(\name{A} \mid \name{B}) &= \frac{p(\name{B} \mid \name{A}) \odot p(\name{A})}{p(\name{B} \mid \name{A}) \ndot{A} p(\name{A})}.
\end{align*}

\subsection{Transformer}
\label{sec:transformer}

We define a Transformer used autoregressively as a language model. The input is a sequence of one-hot vectors $I$, from which we compute word embeddings and positional encodings.
\begin{align*}
  I &\in \{0, 1\}^{\name{seq} \times \name{vocab}} & \nsum{vocab} I &= 1\\
  E &\in \reals^{\name{vocab} \times \name{layer}} \\
  W &= (E \ndot{vocab} I)\sqrt{|\name{layer}|} \\
  P &\in \reals^{\name{seq} \times \name{layer}} \\
  P_{\nidx{seq}{p}, \nidx{layer}{i}} &= \begin{cases}
    \sin((p-1) / 10000^{(i-1) / |\name{layer}|}) & \text{$i$ odd} \\ 
    \cos((p-1) / 10000^{(i-2) / |\name{layer}|}) & \text{$i$ even}
  \end{cases}
\end{align*}
Then we use $L$ layers of self-attention and feed-forward neural networks.
\begin{align*} 
X^0 &= W+P \\
T^1 &= \nfun{layer}{xnorm}(\text{SelfAtt}(X^0)) + X^0\\
X^1 &= \nfun{layer}{xnorm}(\text{FFN}(T^1)) + T^1\\
&\vdotswithin{=} \\
T^{L} &= \nfun{layer}{xnorm}(\text{SelfAtt}(X^{L-1})) + X^{L-1}\\
X^{L} &= \nfun{layer}{xnorm}(\text{FFN}(T^L)) + T^L\\
O &= \nfun{vocab}{softmax}(E \ndot{layer} X^L)
\end{align*}
where $\text{SelfAtt}$ and $\text{FFNN}$ are defined below.

The feedforward neural networks are:
\begin{align*}
  W^1 &\in \mathbb{R}^{\name{hidden}, \name{layer}} & b^1 \in \mathbb{R}^{\name{hidden}} \\
  W^2 &\in \mathbb{R}^{\name{layer}, \name{hidden}} & b^2 \in \mathbb{R}^{\name{layer}} \\
  \text{FFN} &\colon \mathbb{R}^{\name{layer}} \rightarrow \mathbb{R}^{\name{layer}} \\
  \text{FFN}(x; W^1, b^1, W^2, b^2) &= \text{ReLU}(W^2 \ndot{hidden} y + b^2) \\
  y &= \text{ReLU}(W^1 \ndot{layer} x + b^1)
\end{align*}

We defined attention above (\S\ref{sec:attention}); the Transformer uses multi-head self-attention, in which $Q$, $K$, and $V$ are all computed from the same sequence. The parameters are:
\begin{align*}
  |\name{key}| = |\name{val}| &= |\name{layer}|/|\name{heads}| \\
  W^Q &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{key}} \\
  W^K &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{key}} \\
  W^V &\in \mathbb{R}^{\name{heads} \times \name{layer} \times \name{val}} \\
  W^O &\in \mathbb{R}^{\name{heads} \times \name{val} \times \name{layer}}
\end{align*}
Then define
\begin{align*}
  \text{SelfAtt} &\colon \mathbb{R}^{\name{seq} \times \name{layer}} \rightarrow \mathbb{R}^{\name{seq} \times \name{layer}} \\
  \text{SelfAtt}(X; W^Q, W^K, W^V, W^O) &= \nsum{heads} W^O \ndot{val} \nmov{seq'}{seq}{\text{Att}(Q, K, V, M)}
\end{align*}
where
\begin{align*}
  |\name{seq}| &= |\name{seq'}| \\
  Q &= \nmov{seq}{seq'}{W^Q \ndot{layer} X} \\
  K &= W^K \ndot{layer} X \\
  V &= W^V \ndot{layer} X \\
  M & \in \reals^{\name{seq} \times \name{seq'}} \\
  M_{\nidx{seq}{i}, \nidx{seq'}{j}} &= \begin{cases}
    0 & i \leq j\\
    -\infty & \text{otherwise.}
  \end{cases}
\end{align*}

\subsection{LeNet}

Parameters:
\begin{align*} 
W^1 & \in \reals^{\nset{channels'}{c_2} \times \nset{channels}{c_1} \times \nset{kh}{kh_1} \times \nset{kw}{kw_1}} & b^1 \in \reals^{\nset{channels'}{c_2}} \\ 
W^2 & \in \reals^{\nset{channels'}{c_3} \times \nset{channels}{c_2} \times \nset{kh}{kh_2} \times \nset{kw}{kw_2}} & b^2 \in \reals^{\nset{channels'}{c_3}} \\ 
\end{align*}
Model:
\begin{align*}
X^0 &\in \reals^{\name{batch} \times \nset{channels}{c_1} \times \name{height} \times \name{width}}\\
X^1 &= \nmov{channels'}{channels}{\text{ReLU}(\text{conv2d}(X^0; W^1, b^1))} \\
X^2 &= \text{maxpool2d}(X^1, ph_1, ph_2) \\
X^3 &= \nmov{channels'}{channels}{\text{ReLU}(\text{conv2d}(X^2; W^2, b^2))} \\
X^4 &= \text{maxpool2d}(X^3, ph_2, ph_2) \\
X^5 &= \nmov{height,width,channels}{layer}{X^4} \\
H &= \text{ReLU}(W^3 \ndot{layer} X^5 + b^3) \\
O &= \nfun{class}{softmax}(\text{ReLU}(W^4 \ndot{hidden} H + b^4))
\end{align*}
The flattening operation in the equation for $X^5$ is defined in \S{\ref{sec:reshaping}}.

\subsection{Other examples}

\subsubsection{Continuous bag of words}

A continuous bag-of-words model classifies by summing up the embeddings of a sequence of words $X$ and then projecting them to the space of classes. 

\begin{align*}
X &\in \{0, 1\}^{{\name{seq} \times \name{vocab}}} & \nsum{vocab} X &= 1\\
E &\in \reals^{{\name{vocab} \times \name{hidden}}}\\
W &\in \reals^{{\name{classes} \times \name{hidden}}}\\
\text{cbow}(X; E, W) &= \nfun{class}{softmax} (W \ndot{hidden} E \ndot{vocab} X)
\end{align*}
Here, the two contractions can be done in either order, so we leave the parentheses off.

\subsubsection{Sudoku ILP}

Sudoku puzzles can be represented as  binary tiled tensors.
Given a grid we can check that it is valid by converting it to a grid of grids. 
Constraints then ensure that there is one digit per row, per column and per sub-box

\begin{align*} 
X &\in \{0, 1\}^{\nset{height}{9} \times \nset{width}{9} \times \nset{assign}{9}}  \\
\text{check}(X) &=
\left(\nsum{assign} Y = 
\nsum{Height, height} Y = 
\nsum{Width, width} Y =  
\nsum{height, width} Y = 1 \right) \\
Y &\in \{0, 1\}^{\nset{Height}{3} \times \nset{Width}{3} \times \nset{height}{3} \times \nset{width}{3} \times \nset{assign}{9}}  \\
Y_{\nidx{Height}{r}, \nidx{height}{r'}, \nidx{Width}{c}, \nidx{width}{c'}} &= X_{\nidx{height}{r\times 3 + r'-1}, \nidx{width}{c\times3 + c'-1}}
\end{align*} 

\subsubsection{$K$-means clustering}

The following equations define one step of $k$-means clustering. Given a set of points $X$ and an initial set of cluster centers $C$,
\begin{align*}
  X &\in \reals^{\name{batch} \times \name{space}} \\
C &\in \reals^{\name{clusters} \times \name{space}}
\end{align*}
we compute cluster assignments
\begin{align*}
Q &= \nfun{clusters}{argmin} \nfun{space}{norm}(C-X) \\
  &= \lim_{\alpha \rightarrow -\infty} \nfun{clusters}{softmax} \left(\alpha \nfun{space}{norm}(C-X)\right)
\end{align*}
then we recompute the cluster centers:
\begin{equation*}
C \leftarrow \nsum{batch} \frac{Q \odot X}{Q}.
\end{equation*}

\subsubsection{Beam search}

Beam search is a commonly used approach for approximate discrete search. Here $H$ is the score of each element in the beam, $S$ is the state of each element in the beam, and $f$ is an update function that returns the score of each state transition. 
Beam step returns the new $H$ tensor. 

\begin{align*} 
|\name{state}| &= |\name{state'}| \\
H &\in \reals^{{\name{batch} \times \name{beam}}} \\
S &\in \{0, 1\}^{{\name{batch} \times \name{beam} \times \name{state}}} & \nsum{state} S &= 1\\
f &\colon \{0, 1\}^{{\name{state}}} \rightarrow \reals^{{\name{state'}}} \\ 
\text{beamstep}(H, S) &= \nfun{beam, state'}{maxk} \left( \nfun{state'}{softmax}(f(S)) \odot H \right)
\end{align*} 

\subsubsection{Multivariate normal distribution}

In our notation, the application of a bilinear form is more verbose than the standard notation ($(X-\mu)^\top \Sigma^{-1} (X-\mu)$), but also makes it look more like a function of two arguments (and would generalize to three or more arguments).

\begin{align*} 
|\name{d}| &= |\name{d1}| = |\name{d2}| \\
X &\in \reals^{\name{d}}  \\
\mu &\in \reals^{\name{d}}  \\
\Sigma & \in   \reals^{\name{d1} \times \name{d2}}  \\
{\cal N}(X; \mu, \Sigma) &= \frac{\displaystyle \exp\left(-\frac{1}{2}  \left(\nfun{d1, d2}{inv}(\Sigma) \ndot{d1} \nmov{d}{d1}{X - \mu} \right) \ndot{d2} \nmov{d}{d2}{X - \mu} \right)}{\sqrt{(2 \pi)^{|\name{d}|} \nfun{d1, d2}{det}(\Sigma)}}
\end{align*}

\section{\LaTeX{} Macros}

Many of the \LaTeX{} macros used in this document are available in the style file \url{https://namedtensor.github.io/namedtensor.sty}. To use it, put
\begin{quote}
\begin{verbatim}
\usepackage{namedtensor}
\end{verbatim}
\end{quote}
in the preamble of your \LaTeX{} source file (after \verb|\documentclass{article}| but before \verb|\begin{document}|).

The style file contains a small number of macros:
\begin{itemize}
\item Use \verb|\name{foo}| to write an index name: $\name{foo}$.
\item Use \verb|\ndef| to define a named index set: $\name{foo} \ndef [n]$.
\item Use \verb|A \ndot{foo} B| for contraction: $A \ndot{foo} B$. Similarly, use \verb|A \ncat{foo} B| for concatenation.
\item Use \verb|\nsum{foo} A| for summation: $\nsum{foo} A$.
\item Use \verb|\nfun{foo}{qux} A| for a function named qux with a name under it: $\nfun{foo}{qux} A$.
\item Use \verb|\nmov{foo}{bar}{A}| for renaming: $\nmov{foo}{bar}{A}$.
\end{itemize}

\section{Formal Definitions}
\label{sec:definitions}

% ugly hack to get non-sans-serif into names
\newcommand{\sub}[1]{_\text{$#1$}}

\subsection{Records}

A \emph{named index} is a pair, written $\nidx{i}{x}$, where $\name{i}$ is a \emph{name} and $x$ is usually a positive integer. We write both names and variables ranging over names using sans-serif font.

A \emph{record} is a set of named indices $\tuple{\nidx{i\sub{1}}{x_1}, \ldots, \nidx{i\sub{r}}{x_r}}$, where $\name{i\sub{1}}, \ldots \name{i\sub{r}}$ are pairwise distinct names. 

If $\name{i}$ is a name and $X$ is a set, define the \emph{named index set} $\nset{i}{X}$ to be the set $\{\nidx{i}{x} \mid x \in X\}$. We deal with named index sets of the form $\nset{i}{[n]}$ (${}=\nset{i}{\{1, \ldots, n\}}$) so frequently that we abbreviate this as $\nset{i}{n}$.

A \emph{named shape} is a set of named index sets $\{\nset{i\sub{1}}{X_1}, \ldots, \nset{i\sub{r}}{X_r}\}$, where $\name{i\sub{1}}, \ldots \name{i\sub{r}}$ are pairwise distinct names. A named shape defines a set of records:
\begin{equation*}
\tupleshape \{\nset{i\sub{1}}{X_1}, \ldots, \nset{i\sub{r}}{X_r}\} = \left\{\tuple{\nidx{i\sub{1}}{x_1}, \ldots, \nidx{i\sub{r}}{x_r}} \mid x_1 \in X_1, \ldots, x_r \in X_r\right\},
\end{equation*}
and for this reason we often write a shape with the alternative notation $\nset{i\sub{1}}{X_1} \times \cdots \times \nset{i\sub{r}}{X_r}$, because it's like an ``unordered Cartesian product'' of the index sets.

We say two shapes $\mathcal{S}$ and $\mathcal{T}$ are \emph{compatible} if whenever $\nidx{i}{X} \in \mathcal{S}$ and $\nidx{i}{Y} \in \mathcal{T}$, then $X = Y$. We say that $\mathcal{S}$ and $\mathcal{T}$ are \emph{orthogonal} if there is no $\name{i}$ such that $\nidx{i}{X} \in \mathcal{S}$ and $\nidx{i}{Y} \in \mathcal{T}$ for any $X$, $Y$.

If $t \in \tupleshape \mathcal{T}$ and $\mathcal{S} \subseteq \mathcal{T}$, then we write $\tuplerestrict{t}{\mathcal{S}}$ for the unique tuple in $\tupleshape{\mathcal{S}}$ such that $\tuplerestrict{t}{\mathcal{S}} \subseteq t$.

Because it's very common for a name to be always used with the same index set, we can write $\name{i} \ndef X$ to create a named index set $\nset{i}{X}$, and this named index set is \emph{also} called $\name{i}$. The context always makes it clear whether the name or the named index set is meant.

\subsection{Named tensors}

Let $F$ be a field and let $\mathcal{T}$ be a set of records. Then a \emph{named tensor over $F$ with shape $\mathcal{T}$} is a mapping from $\mathcal{T}$ to $F$. We write the set of all named tensors with shape $\mathcal{T}$ as $F^{\mathcal{T}}$.

We don't make any distinction between a scalar (an element of $F$) and a named tensor with empty shape (an element of $F^\emptyset$).

If $A \in F^{\mathcal{T}}$, then we access an element of $A$ by applying it to a record $t \in \tupleshape\mathcal{T}$; but we write this using the usual subscript notation: $A_t$ rather than $A(t)$. To avoid clutter, in place of $A_{\tuple{\nidx{i\sub{1}}{x_1}, \ldots, \nidx{i\sub{r}}{x_r}}}$, we usually write $A_{\nidx{i\sub{1}}{x_1}, \ldots, \nidx{i\sub{r}}{x_r}}$. When a named tensor is an expression like $(A+B)$, we surround it with square brackets like this: $[A+B]_{\nidx{i\sub{1}}{x_1}, \ldots, \nidx{i\sub{r}}{x_r}}$.

We also allow partial indices. If $A$ is a tensor with shape $\mathcal{T}$ and $s \in \tupleshape \mathcal{S}$ where $\mathcal{S} \subseteq \mathcal{T}$, then we define $A_s$ to be the named tensor with shape $\mathcal{T} \setminus \mathcal{S}$ such that, for any $t \in \tupleshape (\mathcal{T} \setminus \mathcal{S})$,
\begin{align*}
\left[A_s\right]_t &= A_{s \cup t}.
\end{align*}
(For the edge case $\mathcal{S} = \mathcal{U}$ and $\mathcal{T} = \emptyset$, our definitions for indexing and partial indexing coincide: one gives a scalar and the other gives a tensor with empty shape, but we don't distinguish between the two.)

\subsection{Extending functions to named tensors}
\label{sec:tensorfunctions}

In \S\ref{sec:intro}, we described several classes of functions that can be extended to named tensors. Here, we define how to do this for general functions.

Let $f \colon F^{\mathcal{S}} \rightarrow G^{\mathcal{T}}$ be a function from tensors to tensors. For any shape $\mathcal{U}$ orthogonal to both $\mathcal{S}$ and $\mathcal{T}$, we can extend $f$ to:
\begin{align*}
f &: F^{\mathcal{S} \cup \mathcal{U}} \rightarrow G^{\mathcal{T} \cup \mathcal{U}} \\
[f(A)]_u &= f(A_u) \qquad \text{for all $u \in \tupleshape\mathcal{U}$.}
\end{align*}

If $f$ is a multary function, we can extend its arguments to larger shapes, and we don't have to extend all the arguments with the same names. We consider just the case of two arguments; three or more arguments are analogous. Let $f \colon F^{\mathcal{S}} \times G^{\mathcal{T}} \rightarrow H^{\mathcal{U}}$ be a binary function from tensors to tensors. For any shapes $\mathcal{S'}$ and $\mathcal{T'}$ that are compatible with each other and orthogonal to $\mathcal{S}$ and $\mathcal{T}$, respectively, and $\mathcal{U'} = \mathcal{S'} \cup \mathcal{T'}$ is orthogonal to $\mathcal{U}$, we can extend $f$ to:
\begin{align*}
f &: F^{\mathcal{S} \cup \mathcal{S'}} \times G^{\mathcal{T} \cup \mathcal{T'}} \rightarrow H^{\mathcal{U} \cup \mathcal{U'}} \\
  [f(A,B)]_u &= f\left(A_{\tuplerestrict{u}{\mathcal{S'}}},B_{\tuplerestrict{u}{\mathcal{T'}}}\right) \qquad \text{for all $u \in \tupleshape\mathcal{U'}$.}
\end{align*}

All of the tensor operations described in \S\ref{sec:operations} can be defined in this way. For example, the contraction operator extends the following ``named dot-product'':
\begin{align*}
\ndot{i} &: F^{\nset{i}{n}} \times F^{\nset{i}{n}} \rightarrow F \\
A \ndot{i} B &= \sum_{i=1}^n A_{\nidx{i}{i}} B_{\nidx{i}{i}}.
\end{align*}

\section{Extensions}

\input{types}
\input{dual}
\input{fancy}

\section*{Acknowledgements}

Thanks to Ekin Aky\"{u}rek, Colin McDonald, Chung-chieh Shan, and Nishant Sinha for their input to this document (or the ideas in it).

\iffalse % hack to make this heading appear only in pandoc
\section*{References}
\fi

\bibliographystyle{acl_natbib}
\bibliography{references}

\end{document}
